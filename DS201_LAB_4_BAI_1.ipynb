{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGw-XzywRZ0N",
        "outputId": "d4d8a3da-897d-46a7-d247-9d2b449b5211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_vocab.py\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, path, src_language, tgt_language, min_freq=5):\n",
        "        self.src_language = src_language\n",
        "        self.tgt_language = tgt_language\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.unk_token = \"<unk>\"\n",
        "        self.bos_token = \"<bos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "\n",
        "        self.pad_idx = 0\n",
        "        self.unk_idx = 1\n",
        "        self.bos_idx = 2\n",
        "        self.eos_idx = 3\n",
        "\n",
        "        self.src_s2i = {self.pad_token: self.pad_idx, self.unk_token: self.unk_idx}\n",
        "        self.src_i2s = {self.pad_idx: self.pad_token, self.unk_idx: self.unk_token}\n",
        "        self.tgt_s2i = {self.pad_token: self.pad_idx, self.unk_token: self.unk_idx, self.bos_token: self.bos_idx, self.eos_token: self.eos_idx}\n",
        "        self.tgt_i2s = {self.pad_idx: self.pad_token, self.unk_idx: self.unk_token, self.bos_idx: self.bos_token, self.eos_idx: self.eos_token}\n",
        "\n",
        "        self.build_vocab(path)\n",
        "\n",
        "    def load_data(self, path):\n",
        "        files = [\"small-train.json\", \"small-dev.json\", \"small-test.json\"]\n",
        "        data = []\n",
        "        for file in files:\n",
        "            full_path = os.path.join(path, file)\n",
        "            if os.path.exists(full_path):\n",
        "                with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    data.extend(json.load(f))\n",
        "        return data\n",
        "\n",
        "    def build_vocab(self, path):\n",
        "        data = self.load_data(path)\n",
        "\n",
        "        src_tokens = [item[self.src_language].split() for item in data]\n",
        "        tgt_tokens = [item[self.tgt_language].split() for item in data]\n",
        "\n",
        "        src_counter = Counter(chain.from_iterable(src_tokens))\n",
        "        tgt_counter = Counter(chain.from_iterable(tgt_tokens))\n",
        "\n",
        "        # Xây dựng từ điển Source (tiếng Anh)\n",
        "        for token, count in src_counter.items():\n",
        "            if count >= self.min_freq and token not in self.src_s2i:\n",
        "                idx = len(self.src_s2i)\n",
        "                self.src_s2i[token] = idx\n",
        "                self.src_i2s[idx] = token\n",
        "\n",
        "        # Xây dựng từ điển Target (tiếng Việt)\n",
        "        for token, count in tgt_counter.items():\n",
        "            if count >= self.min_freq and token not in self.tgt_s2i:\n",
        "                idx = len(self.tgt_s2i)\n",
        "                self.tgt_s2i[token] = idx\n",
        "                self.tgt_i2s[idx] = token\n",
        "\n",
        "        self.src_vocab_size = len(self.src_s2i)\n",
        "        self.tgt_vocab_size = len(self.tgt_s2i)\n",
        "\n",
        "    def encode(self, text, is_target=False):\n",
        "        tokens = text.split()\n",
        "        s2i = self.tgt_s2i if is_target else self.src_s2i\n",
        "        unk_idx = self.unk_idx\n",
        "\n",
        "        indices = [s2i.get(token, unk_idx) for token in tokens]\n",
        "\n",
        "        if is_target:\n",
        "            indices = [self.bos_idx] + indices + [self.eos_idx]\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def decode(self, indices, is_target=False):\n",
        "        i2s = self.tgt_i2s if is_target else self.src_i2s\n",
        "        tokens = [i2s.get(idx, self.unk_token) for idx in indices]\n",
        "        return \" \".join(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFQWWo9-r5LF",
        "outputId": "1dd92acf-d77f-4624-81f4-4840a12fe607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_vocab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_dataset.py\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Đảm bảo có thể import Vocab nếu file này được chạy độc lập\n",
        "if __name__ != \"__main__\":\n",
        "    from phomt_vocab import Vocab\n",
        "else:\n",
        "    # Thêm đường dẫn dự án để import được Vocab khi chạy file này độc lập\n",
        "    sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
        "    try:\n",
        "        from phomt_vocab import Vocab\n",
        "    except ImportError:\n",
        "        print(\"Không tìm thấy phomt_vocab.py. Đảm bảo nó cùng thư mục.\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "class phoMTDataset(Dataset):\n",
        "    def __init__(self, data_path, vocab):\n",
        "        self.vocab = vocab\n",
        "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        src_text = item[self.vocab.src_language]\n",
        "        tgt_text = item[self.vocab.tgt_language]\n",
        "\n",
        "        # Chuyển đổi thành indices\n",
        "        src_indices = self.vocab.encode(src_text, is_target=False)\n",
        "        tgt_indices = self.vocab.encode(tgt_text, is_target=True)\n",
        "\n",
        "        return {\n",
        "            'src': torch.tensor(src_indices, dtype=torch.long),\n",
        "            'tgt': torch.tensor(tgt_indices, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Tìm chiều dài lớn nhất của câu trong batch\n",
        "    src_lens = [len(item['src']) for item in batch]\n",
        "    tgt_lens = [len(item['tgt']) for item in batch]\n",
        "    max_src_len = max(src_lens)\n",
        "    max_tgt_len = max(tgt_lens)\n",
        "\n",
        "    # Lấy pad index (giả định pad_idx là 0)\n",
        "    pad_idx = 0\n",
        "\n",
        "    # Padding\n",
        "    padded_src = torch.full((len(batch), max_src_len), pad_idx, dtype=torch.long)\n",
        "    padded_tgt = torch.full((len(batch), max_tgt_len), pad_idx, dtype=torch.long)\n",
        "\n",
        "    for i, item in enumerate(batch):\n",
        "        padded_src[i, :src_lens[i]] = item['src']\n",
        "        padded_tgt[i, :tgt_lens[i]] = item['tgt']\n",
        "\n",
        "    return {\n",
        "        'src': padded_src,\n",
        "        'tgt': padded_tgt\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAPtXA2Tsdh-",
        "outputId": "3e939260-514e-4d7f-84cf-e60f1bfaf05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/seq2seq_basic_lstm.py\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_encoder, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_encoder = n_encoder\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "\n",
        "        # LSTM Unidirectional: num_layers=n_encoder, batch_first=True\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=d_model,\n",
        "            hidden_size=d_model,\n",
        "            num_layers=n_encoder,\n",
        "            dropout=dropout if n_encoder > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=False # Chỉ sử dụng Unidirectional\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (batch_size, src_len)\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: (batch_size, src_len, d_model)\n",
        "\n",
        "        # output: (batch_size, src_len, d_model * num_directions)\n",
        "        # (h_n, c_n): (num_layers * num_directions, batch_size, d_model)\n",
        "        output, (h_n, c_n) = self.lstm(embedded)\n",
        "\n",
        "        # Do là Unidirectional, h_n và c_n đã có shape đúng: (n_encoder, batch_size, d_model)\n",
        "        return output, (h_n, c_n)\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_decoder, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_decoder = n_decoder\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=d_model,\n",
        "            hidden_size=d_model,\n",
        "            num_layers=n_decoder,\n",
        "            dropout=dropout if n_decoder > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=False # Chỉ sử dụng Unidirectional\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, encoder_hidden):\n",
        "        # tgt: (batch_size, tgt_len) - Decoder input (ví dụ: <bos> y1 y2 ...)\n",
        "        # encoder_hidden: (h_n, c_n) từ Encoder, shape: (n_decoder, batch_size, d_model)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(tgt))\n",
        "        # embedded: (batch_size, tgt_len, d_model)\n",
        "\n",
        "        # output: (batch_size, tgt_len, d_model)\n",
        "        output, hidden = self.lstm(embedded, encoder_hidden)\n",
        "\n",
        "        # output_layer: (batch_size, tgt_len, vocab_size)\n",
        "        prediction = self.output_layer(output)\n",
        "\n",
        "        return prediction, hidden\n",
        "\n",
        "class Seq2SeqLSTM(nn.Module):\n",
        "    def __init__(self, d_model, n_encoder, n_decoder, dropout, vocab):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.encoder = EncoderLSTM(\n",
        "            vocab_size=vocab.src_vocab_size,\n",
        "            d_model=d_model,\n",
        "            n_encoder=n_encoder,\n",
        "            dropout=dropout,\n",
        "            pad_idx=vocab.pad_idx\n",
        "        )\n",
        "\n",
        "        self.decoder = DecoderLSTM(\n",
        "            vocab_size=vocab.tgt_vocab_size,\n",
        "            d_model=d_model,\n",
        "            n_decoder=n_decoder,\n",
        "            dropout=dropout,\n",
        "            pad_idx=vocab.pad_idx\n",
        "        )\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # src: (batch_size, src_len)\n",
        "        # tgt: (batch_size, tgt_len)\n",
        "\n",
        "        _, encoder_hidden = self.encoder(src)\n",
        "        # encoder_hidden là (h_n, c_n) từ Encoder: (n_layers, batch_size, d_model)\n",
        "\n",
        "        # Decoder sử dụng trạng thái cuối cùng của Encoder làm trạng thái ẩn ban đầu\n",
        "        output, _ = self.decoder(tgt, encoder_hidden)\n",
        "        # output: (batch_size, tgt_len, vocab_size)\n",
        "\n",
        "        return output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, src, max_len=50):\n",
        "        self.eval()\n",
        "        batch_size = src.shape[0]\n",
        "\n",
        "        # 1. Encoding\n",
        "        _, hidden = self.encoder(src)\n",
        "\n",
        "        # 2. Decoding - Bắt đầu với <bos> token\n",
        "        # input_token: (batch_size, 1) chứa <bos>\n",
        "        input_token = torch.full((batch_size, 1), self.vocab.bos_idx, dtype=torch.long, device=self.device)\n",
        "\n",
        "        # Tensor để lưu trữ các token được sinh ra\n",
        "        output_tokens = torch.zeros((batch_size, max_len), dtype=torch.long, device=self.device)\n",
        "        output_tokens[:, 0] = self.vocab.bos_idx # Giữ lại <bos> ở vị trí đầu tiên\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            # output: (batch_size, 1, vocab_size), hidden: state mới\n",
        "            output, hidden = self.decoder(input_token, hidden)\n",
        "\n",
        "            # Lấy token có xác suất cao nhất: next_token (batch_size, 1)\n",
        "            next_token = output.argmax(dim=-1)\n",
        "\n",
        "            # Lưu token vào tensor kết quả\n",
        "            output_tokens[:, t] = next_token.squeeze(-1)\n",
        "\n",
        "            # Dừng nếu tất cả các câu trong batch đã sinh ra <eos>\n",
        "            if ((output_tokens[:, t] == self.vocab.eos_idx) | (output_tokens[:, t] == self.vocab.pad_idx)).all():\n",
        "                break\n",
        "\n",
        "            # Cập nhật input cho bước thời gian tiếp theo\n",
        "            input_token = next_token # (batch_size, 1)\n",
        "\n",
        "        return output_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMj8H8UPsiqO",
        "outputId": "1f791f9b-3f70-421c-df2b-e317ad136fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/seq2seq_basic_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/train_basic_lstm.py\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from phomt_dataset import collate_fn, phoMTDataset\n",
        "from phomt_vocab import Vocab\n",
        "from seq2seq_basic_lstm import Seq2SeqLSTM\n",
        "\n",
        "# Import Metrics\n",
        "try:\n",
        "    from torchmetrics.text.rouge import ROUGEScore\n",
        "except ImportError:\n",
        "    # Trường hợp không cài đặt torchmetrics\n",
        "    print(\"Vui lòng cài đặt torchmetrics: pip install torchmetrics\")\n",
        "    exit()\n",
        "\n",
        "# --- Config ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# CẬP NHẬT ĐƯỜNG DẪN CHECKPOINT\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/MODEL-BAI1/\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/DATASET/small-PhoMT/\"\n",
        "\n",
        "\n",
        "# 1. Khởi tạo logger\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "# Xóa các handler cũ nếu có\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "# 2. Định dạng cho Log File\n",
        "file_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "file_handler = logging.FileHandler(os.path.join(CHECKPOINT_DIR, \"training.log\"), mode='a')\n",
        "file_handler.setFormatter(file_formatter)\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "# 3. Định dạng cho Console\n",
        "console_formatter = logging.Formatter(\"%(message)s\")\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setFormatter(console_formatter)\n",
        "logger.addHandler(console_handler)\n",
        "# --- Kết thúc Logging Setup MỚI ---\n",
        "\n",
        "def indices_to_text(indices, vocab, is_target=True):\n",
        "    tokens = []\n",
        "    i2s = vocab.tgt_i2s if is_target else vocab.src_i2s\n",
        "\n",
        "    for idx in indices:\n",
        "        if isinstance(idx, torch.Tensor):\n",
        "            idx = idx.item()\n",
        "\n",
        "        if is_target and idx == vocab.eos_idx:\n",
        "            break\n",
        "\n",
        "        if idx != vocab.pad_idx:\n",
        "            if is_target and idx == vocab.bos_idx:\n",
        "                continue\n",
        "\n",
        "            token = i2s.get(idx, vocab.unk_token)\n",
        "            tokens.append(token)\n",
        "\n",
        "        if not is_target and idx == vocab.eos_idx:\n",
        "            break\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def train(model: nn.Module, dataloader: DataLoader, epoch: int, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    running_loss = []\n",
        "\n",
        "    with tqdm(dataloader, desc=f\"Epoch {epoch} - Training\") as pbar:\n",
        "        for item in pbar:\n",
        "            src = item['src'].to(device)\n",
        "            tgt = item['tgt'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            decoder_input = tgt[:, :-1]\n",
        "            targets = tgt[:, 1:]\n",
        "\n",
        "            logits = model(src, decoder_input)\n",
        "\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), targets.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "            pbar.set_postfix({\"loss\": np.mean(running_loss)})\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = np.mean(running_loss)\n",
        "    logging.info(f\"--- Epoch {epoch} TRAIN finished --- Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate(model: nn.Module, dataloader: DataLoader, epoch: int, loss_fn, vocab):\n",
        "    model.eval()\n",
        "    running_loss = []\n",
        "    rouge_metric = ROUGEScore(rouge_keys=(\"rougeL\",)).to(device)\n",
        "    all_preds_text = []\n",
        "    all_targets_text = []\n",
        "\n",
        "    example_printed = False\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for item in tqdm(dataloader, desc=f\"Epoch {epoch} - Evaluating\"):\n",
        "            src = item['src'].to(device)\n",
        "            tgt = item['tgt'].to(device)\n",
        "\n",
        "            # 1. Validation Loss (Sử dụng Teacher Forcing)\n",
        "            decoder_input = tgt[:, :-1]\n",
        "            targets = tgt[:, 1:]\n",
        "            logits = model(src, decoder_input)\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), targets.reshape(-1))\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            # 2. ROUGE-L Prediction (Sử dụng Inference)\n",
        "            generated_tokens = model.predict(src, max_len=tgt.shape[1] + 10)\n",
        "\n",
        "            for i in range(len(tgt)):\n",
        "                pred_seq = generated_tokens[i].tolist()\n",
        "                pred_text = indices_to_text(pred_seq, vocab, is_target=True)\n",
        "\n",
        "                tgt_seq = tgt[i].tolist()\n",
        "                tgt_text = indices_to_text(tgt_seq, vocab, is_target=True)\n",
        "\n",
        "                # LOGIC IN VÍ DỤ\n",
        "                if not example_printed:\n",
        "                    src_seq = src[i].tolist()\n",
        "                    src_text = indices_to_text(src_seq, vocab, is_target=False)\n",
        "\n",
        "                    logging.info(f\"\\n======== Example Translation (Epoch {epoch}) ========\")\n",
        "                    logging.info(f\"-> Source (EN):     {src_text}\")\n",
        "                    logging.info(f\"-> Reference (VN):  {tgt_text}\")\n",
        "                    logging.info(f\"-> Prediction (VN): {pred_text}\")\n",
        "                    logging.info(\"==================================================\")\n",
        "                    example_printed = True\n",
        "\n",
        "                all_preds_text.append(pred_text)\n",
        "                all_targets_text.append(tgt_text)\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Tính ROUGE trên tập Validation\n",
        "    if len(all_preds_text) > 0:\n",
        "        rouge_scores = rouge_metric(all_preds_text, all_targets_text)\n",
        "        rouge_l = rouge_scores['rougeL_fmeasure'].item()\n",
        "    else:\n",
        "        rouge_l = 0.0\n",
        "\n",
        "    avg_loss = np.mean(running_loss)\n",
        "    logging.info(f\"--- Epoch {epoch} EVAL finished --- Val Loss: {avg_loss:.4f} | ROUGE-L: {rouge_l:.4f}\")\n",
        "\n",
        "    return avg_loss, rouge_l\n",
        "\n",
        "def visualize_metrics(train_losses, val_losses, rouge_scores):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
        "    plt.plot(epochs, val_losses, label='Val Loss', marker='s')\n",
        "    plt.title(\"Loss History\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, rouge_scores, label='Val ROUGE-L', marker='^', color='green')\n",
        "    plt.title(\"ROUGE-L Score History\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    logging.info(\"=\"*50)\n",
        "    logging.info(f\"Starting training on Device: {device}\")\n",
        "\n",
        "    vocab = Vocab(\n",
        "        path=DATASET_ROOT,\n",
        "        src_language=\"english\",\n",
        "        tgt_language=\"vietnamese\"\n",
        "    )\n",
        "\n",
        "    # ĐỌC DATASET\n",
        "    train_dataset = phoMTDataset(os.path.join(DATASET_ROOT, \"small-train.json\"), vocab)\n",
        "    dev_dataset = phoMTDataset(os.path.join(DATASET_ROOT, \"small-dev.json\"), vocab)\n",
        "    test_dataset = phoMTDataset(os.path.join(DATASET_ROOT, \"small-test.json\"), vocab)\n",
        "\n",
        "    logging.info(f\"Using full datasets: Train size={len(train_dataset)}, Dev size={len(dev_dataset)}, Test size={len(test_dataset)}\")\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Khởi tạo model với thông số yêu cầu\n",
        "    model = Seq2SeqLSTM(\n",
        "        d_model=256, # Hidden Size 256\n",
        "        n_encoder=3, # 3 lớp Encoder\n",
        "        n_decoder=3, # 3 lớp Decoder\n",
        "        dropout=0.3,\n",
        "        vocab=vocab\n",
        "    ).to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    logging.info(f\"Model Parameters (LSTM Unidirectional): {total_params:,}\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)\n",
        "\n",
        "    # Đổi tên checkpoint file\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, \"best_basic_lstm_mt.pt\")\n",
        "    best_rouge = 0.0\n",
        "    start_epoch = 0\n",
        "\n",
        "    # Tải Checkpoint nếu tồn tại\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "            model.load_state_dict(ckpt['model_state_dict'])\n",
        "            optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "            start_epoch = ckpt['epoch']\n",
        "            best_rouge = ckpt.get('best_rouge', 0.0)\n",
        "            logging.info(f\"Resumed from epoch {start_epoch}, Best ROUGE: {best_rouge:.4f}\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n",
        "\n",
        "    train_losses, val_losses, val_rouges = [], [], []\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(start_epoch + 1, 20):\n",
        "        logging.info(f\"\\n--- Starting Epoch {epoch} ---\")\n",
        "\n",
        "        t_loss = train(model, train_loader, epoch, loss_fn, optimizer)\n",
        "        v_loss, v_rouge = evaluate(model, dev_loader, epoch, loss_fn, vocab)\n",
        "\n",
        "        train_losses.append(t_loss)\n",
        "        val_losses.append(v_loss)\n",
        "        val_rouges.append(v_rouge)\n",
        "\n",
        "        if v_rouge > best_rouge:\n",
        "            best_rouge = v_rouge\n",
        "            patience = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_rouge': best_rouge\n",
        "            }, checkpoint_path)\n",
        "            # THAY ĐỔI THÔNG BÁO SAVE CHECKPOINT\n",
        "            logging.info(f\"!!! NEW BEST MODEL (Epoch {epoch}) !!! Saved ROUGE-L: {best_rouge:.4f}\")\n",
        "        else:\n",
        "            patience += 1\n",
        "            logging.info(f\"No improvement. Patience: {patience}/10\")\n",
        "            if patience >= 10:\n",
        "                logging.info(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    logging.info(\"\\n================= Final Test Evaluation =================\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "\n",
        "        test_loss, test_rouge = evaluate(model, test_loader, 0, loss_fn, vocab)\n",
        "        logging.info(f\"Final Test Loss: {test_loss:.4f} | Test ROUGE-L: {test_rouge:.4f}\")\n",
        "    else:\n",
        "        logging.warning(\"Cannot evaluate on Test Set: Best model checkpoint not found.\")\n",
        "\n",
        "    visualize_metrics(train_losses, val_losses, val_rouges)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        # Sử dụng logger để ghi lỗi vào file\n",
        "        logger.error(f\"Error: {str(e)}\", exc_info=True)\n",
        "        # In thông báo lỗi ngắn gọn ra console\n",
        "        print(f\"\\n[FATAL ERROR] Check training.log for details. Error: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZGBk12Fs2J4",
        "outputId": "8edc3118-16d1-491e-c44f-30a374c844b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/train_basic_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-6BJVMhpxofQ",
        "outputId": "fd171d7f-8b7e-4e76-9af8-0f888eb86c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/train_basic_lstm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdITRUqavTnX",
        "outputId": "68828fa2-ce31-445e-f5d2-32966467d2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Starting training on Device: cuda\n",
            "Using full datasets: Train size=20000, Dev size=2000, Test size=2000\n",
            "Model Parameters (LSTM Unidirectional): 6,563,783\n",
            "\n",
            "--- Starting Epoch 1 ---\n",
            "Epoch 1 - Training: 100% 1250/1250 [00:27<00:00, 45.07it/s, loss=5.66]\n",
            "--- Epoch 1 TRAIN finished --- Loss: 5.6633\n",
            "Epoch 1 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 1) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): Tôi có thể làm một người , và chúng ta có thể làm một người , và chúng ta có thể làm một người , và chúng ta có thể làm một người , và chúng ta có thể làm một người .\n",
            "==================================================\n",
            "Epoch 1 - Evaluating: 100% 125/125 [00:04<00:00, 30.54it/s]\n",
            "--- Epoch 1 EVAL finished --- Val Loss: 5.5259 | ROUGE-L: 0.2823\n",
            "!!! NEW BEST MODEL (Epoch 1) !!! Saved ROUGE-L: 0.2823\n",
            "\n",
            "--- Starting Epoch 2 ---\n",
            "Epoch 2 - Training: 100% 1250/1250 [00:29<00:00, 41.79it/s, loss=5.04]\n",
            "--- Epoch 2 TRAIN finished --- Loss: 5.0357\n",
            "Epoch 2 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 2) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): Chúng ta có thể làm việc , nhưng chúng ta có thể làm việc , và tôi có thể làm việc , và tôi có thể làm việc , và tôi có thể làm việc , và tôi có thể làm việc .\n",
            "==================================================\n",
            "Epoch 2 - Evaluating: 100% 125/125 [00:05<00:00, 23.75it/s]\n",
            "--- Epoch 2 EVAL finished --- Val Loss: 5.1959 | ROUGE-L: 0.2546\n",
            "No improvement. Patience: 1/10\n",
            "\n",
            "--- Starting Epoch 3 ---\n",
            "Epoch 3 - Training: 100% 1250/1250 [00:27<00:00, 45.05it/s, loss=4.72]\n",
            "--- Epoch 3 TRAIN finished --- Loss: 4.7224\n",
            "Epoch 3 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 3) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): Chúng ta có thể làm được những gì chúng ta có thể làm được một điều mà chúng ta có thể làm được những gì chúng ta có thể làm được những gì chúng ta có thể làm được những gì chúng ta có thể làm được .\n",
            "==================================================\n",
            "Epoch 3 - Evaluating: 100% 125/125 [00:05<00:00, 21.20it/s]\n",
            "--- Epoch 3 EVAL finished --- Val Loss: 4.9905 | ROUGE-L: 0.2687\n",
            "No improvement. Patience: 2/10\n",
            "\n",
            "--- Starting Epoch 4 ---\n",
            "Epoch 4 - Training: 100% 1250/1250 [00:27<00:00, 44.93it/s, loss=4.51]\n",
            "--- Epoch 4 TRAIN finished --- Loss: 4.5103\n",
            "Epoch 4 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 4) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , chúng ta có thể làm được những gì chúng ta có thể làm được , và tôi sẽ nói với bạn rằng , \" Tôi không thể làm gì đó . \"\n",
            "==================================================\n",
            "Epoch 4 - Evaluating: 100% 125/125 [00:05<00:00, 24.51it/s]\n",
            "--- Epoch 4 EVAL finished --- Val Loss: 4.8529 | ROUGE-L: 0.2125\n",
            "No improvement. Patience: 3/10\n",
            "\n",
            "--- Starting Epoch 5 ---\n",
            "Epoch 5 - Training: 100% 1250/1250 [00:27<00:00, 44.89it/s, loss=4.35]\n",
            "--- Epoch 5 TRAIN finished --- Loss: 4.3476\n",
            "Epoch 5 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 5) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 5 - Evaluating: 100% 125/125 [00:05<00:00, 24.64it/s]\n",
            "--- Epoch 5 EVAL finished --- Val Loss: 4.7677 | ROUGE-L: 0.2251\n",
            "No improvement. Patience: 4/10\n",
            "\n",
            "--- Starting Epoch 6 ---\n",
            "Epoch 6 - Training: 100% 1250/1250 [00:27<00:00, 45.17it/s, loss=4.23]\n",
            "--- Epoch 6 TRAIN finished --- Loss: 4.2292\n",
            "Epoch 6 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 6) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , người ta đã làm việc với những người khác , và chúng ta sẽ không thể làm gì đó , và tôi sẽ nói về những gì chúng ta có thể làm gì đó .\n",
            "==================================================\n",
            "Epoch 6 - Evaluating: 100% 125/125 [00:05<00:00, 21.18it/s]\n",
            "--- Epoch 6 EVAL finished --- Val Loss: 4.7092 | ROUGE-L: 0.2690\n",
            "No improvement. Patience: 5/10\n",
            "\n",
            "--- Starting Epoch 7 ---\n",
            "Epoch 7 - Training: 100% 1250/1250 [00:27<00:00, 45.42it/s, loss=4.13]\n",
            "--- Epoch 7 TRAIN finished --- Loss: 4.1312\n",
            "Epoch 7 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 7) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 7 - Evaluating: 100% 125/125 [00:05<00:00, 24.54it/s]\n",
            "--- Epoch 7 EVAL finished --- Val Loss: 4.6723 | ROUGE-L: 0.2374\n",
            "No improvement. Patience: 6/10\n",
            "\n",
            "--- Starting Epoch 8 ---\n",
            "Epoch 8 - Training: 100% 1250/1250 [00:27<00:00, 45.40it/s, loss=4.05]\n",
            "--- Epoch 8 TRAIN finished --- Loss: 4.0464\n",
            "Epoch 8 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 8) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 8 - Evaluating: 100% 125/125 [00:05<00:00, 22.66it/s]\n",
            "--- Epoch 8 EVAL finished --- Val Loss: 4.6340 | ROUGE-L: 0.2527\n",
            "No improvement. Patience: 7/10\n",
            "\n",
            "--- Starting Epoch 9 ---\n",
            "Epoch 9 - Training: 100% 1250/1250 [00:27<00:00, 45.26it/s, loss=3.97]\n",
            "--- Epoch 9 TRAIN finished --- Loss: 3.9738\n",
            "Epoch 9 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 9) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 9 - Evaluating: 100% 125/125 [00:05<00:00, 23.06it/s]\n",
            "--- Epoch 9 EVAL finished --- Val Loss: 4.6193 | ROUGE-L: 0.2594\n",
            "No improvement. Patience: 8/10\n",
            "\n",
            "--- Starting Epoch 10 ---\n",
            "Epoch 10 - Training: 100% 1250/1250 [00:27<00:00, 45.38it/s, loss=3.91]\n",
            "--- Epoch 10 TRAIN finished --- Loss: 3.9105\n",
            "Epoch 10 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 10) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 10 - Evaluating: 100% 125/125 [00:05<00:00, 24.81it/s]\n",
            "--- Epoch 10 EVAL finished --- Val Loss: 4.5908 | ROUGE-L: 0.2508\n",
            "No improvement. Patience: 9/10\n",
            "\n",
            "--- Starting Epoch 11 ---\n",
            "Epoch 11 - Training: 100% 1250/1250 [00:27<00:00, 45.38it/s, loss=3.85]\n",
            "--- Epoch 11 TRAIN finished --- Loss: 3.8541\n",
            "Epoch 11 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 11) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 11 - Evaluating: 100% 125/125 [00:05<00:00, 21.05it/s]\n",
            "--- Epoch 11 EVAL finished --- Val Loss: 4.5888 | ROUGE-L: 0.2788\n",
            "No improvement. Patience: 10/10\n",
            "Early stopping!\n",
            "\n",
            "================= Final Test Evaluation =================\n",
            "Epoch 0 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 0) ========\n",
            "-> Source (EN):     Brother Albert <unk> and his wife , <unk> Susan <unk> , from the West <unk> in <unk> , <unk>\n",
            "-> Reference (VN):  Anh Albert <unk> và chị Susan <unk> , thuộc hội thánh West ở <unk> , <unk>\n",
            "-> Prediction (VN): Tôi có thể thấy một người , và tôi có thể làm một người , và chúng ta có thể làm một người , và chúng ta có thể làm một người , và chúng ta có thể làm một người .\n",
            "==================================================\n",
            "Epoch 0 - Evaluating: 100% 125/125 [00:04<00:00, 27.35it/s]\n",
            "--- Epoch 0 EVAL finished --- Val Loss: 5.4340 | ROUGE-L: 0.2850\n",
            "Final Test Loss: 5.4340 | Test ROUGE-L: 0.2850\n",
            "Figure(1500x600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nhận xét chung:**\n",
        "- Val Loss: Loss trên tập Validation giảm đều từ $5.5259$ (E1) xuống $4.5888$ (E11). Điều này cho thấy mô hình đang học và hội tụ.\n",
        "- ROUGE-L: Mặc dù Loss giảm, điểm ROUGE-L dao động quanh mức $0.25 - 0.28$ và không thể cải thiện vượt qua mức $0.2823$ của Epoch 1. Đây là dấu hiệu rõ ràng của một vấn đề về kiến trúc.\n",
        "- Early Stopping: Quá trình dừng sớm diễn ra ở Epoch 11 sau khi không có cải thiện ROUGE-L trong 10 Epoch liên tiếp (Patience: 10/10)."
      ],
      "metadata": {
        "id": "4G9C1ksIlVVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nguyên nhân chính**: Information Bottleneck (Nút thắt thông tin)\n",
        "\n",
        "Mô hình Seq2Seq LSTM cơ bản (không có Attention) yêu cầu Encoder nén toàn bộ thông tin của câu nguồn (dài khoảng 20-30 từ) vào một vector ngữ cảnh cố định ($h_n, c_n$).\n",
        "1. Mất thông tin: Khi câu nguồn quá dài (như ví dụ về cơn bão), vector ngữ cảnh không thể lưu trữ đầy đủ chi tiết.\n",
        "2. Khó khăn trong Decoding: Decoder không có khả năng \"nhìn lại\" từng phần của câu nguồn. Nó phải dựa vào vector ngữ cảnh bị tắc nghẽn này, dẫn đến:\n",
        "- Lặp từ: Mô hình dễ dàng rơi vào các trạng thái lặp lại đơn giản để kéo dài câu.\n",
        "- Mất ngữ cảnh: Bản dịch hoàn toàn mất đi ý nghĩa của câu gốc."
      ],
      "metadata": {
        "id": "6g9BWdw4lhdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giải pháp:** Chuyển sang sử dụng mô hình Seq2Seq LSTM với cơ chế Attention (Cụ thể là Luông Attention hoặc Bahdanau Attention). Attention giúp Decoder \"chọn lọc\" các từ quan trọng trong câu nguồn ở mỗi bước dịch, giải quyết triệt để vấn đề \"nút thắt thông tin\" của kiến trúc cơ bản."
      ],
      "metadata": {
        "id": "Unqd_Of3lxfz"
      }
    }
  ]
}