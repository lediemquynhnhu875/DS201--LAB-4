{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGw-XzywRZ0N",
        "outputId": "50c2a947-486f-44bf-a3f8-8ae61ad66d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_vocab.py\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, path, src_language, tgt_language, min_freq=5):\n",
        "        self.src_language = src_language\n",
        "        self.tgt_language = tgt_language\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.unk_token = \"<unk>\"\n",
        "        self.bos_token = \"<bos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "\n",
        "        self.pad_idx = 0\n",
        "        self.unk_idx = 1\n",
        "        self.bos_idx = 2\n",
        "        self.eos_idx = 3\n",
        "\n",
        "        self.src_s2i = {self.pad_token: self.pad_idx, self.unk_token: self.unk_idx}\n",
        "        self.src_i2s = {self.pad_idx: self.pad_token, self.unk_idx: self.unk_token}\n",
        "        self.tgt_s2i = {self.pad_token: self.pad_idx, self.unk_token: self.unk_idx, self.bos_token: self.bos_idx, self.eos_token: self.eos_idx}\n",
        "        self.tgt_i2s = {self.pad_idx: self.pad_token, self.unk_idx: self.unk_token, self.bos_idx: self.bos_token, self.eos_idx: self.eos_token}\n",
        "\n",
        "        self.build_vocab(path)\n",
        "\n",
        "    def load_data(self, path):\n",
        "        files = [\"small-train.json\", \"small-dev.json\", \"small-test.json\"]\n",
        "        data = []\n",
        "        for file in files:\n",
        "            full_path = os.path.join(path, file)\n",
        "            if os.path.exists(full_path):\n",
        "                with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    data.extend(json.load(f))\n",
        "        return data\n",
        "\n",
        "    def build_vocab(self, path):\n",
        "        data = self.load_data(path)\n",
        "\n",
        "        src_tokens = [item[self.src_language].split() for item in data]\n",
        "        tgt_tokens = [item[self.tgt_language].split() for item in data]\n",
        "\n",
        "        src_counter = Counter(chain.from_iterable(src_tokens))\n",
        "        tgt_counter = Counter(chain.from_iterable(tgt_tokens))\n",
        "\n",
        "        # Xây dựng từ điển Source (tiếng Anh)\n",
        "        for token, count in src_counter.items():\n",
        "            if count >= self.min_freq and token not in self.src_s2i:\n",
        "                idx = len(self.src_s2i)\n",
        "                self.src_s2i[token] = idx\n",
        "                self.src_i2s[idx] = token\n",
        "\n",
        "        # Xây dựng từ điển Target (tiếng Việt)\n",
        "        for token, count in tgt_counter.items():\n",
        "            if count >= self.min_freq and token not in self.tgt_s2i:\n",
        "                idx = len(self.tgt_s2i)\n",
        "                self.tgt_s2i[token] = idx\n",
        "                self.tgt_i2s[idx] = token\n",
        "\n",
        "        self.src_vocab_size = len(self.src_s2i)\n",
        "        self.tgt_vocab_size = len(self.tgt_s2i)\n",
        "\n",
        "    def encode(self, text, is_target=False):\n",
        "        tokens = text.split()\n",
        "        s2i = self.tgt_s2i if is_target else self.src_s2i\n",
        "        unk_idx = self.unk_idx\n",
        "\n",
        "        indices = [s2i.get(token, unk_idx) for token in tokens]\n",
        "\n",
        "        if is_target:\n",
        "            indices = [self.bos_idx] + indices + [self.eos_idx]\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def decode(self, indices, is_target=False):\n",
        "        i2s = self.tgt_i2s if is_target else self.src_i2s\n",
        "        tokens = [i2s.get(idx, self.unk_token) for idx in indices]\n",
        "        return \" \".join(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFQWWo9-r5LF",
        "outputId": "1dd92acf-d77f-4624-81f4-4840a12fe607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_vocab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_dataset.py\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Đảm bảo có thể import Vocab nếu file này được chạy độc lập\n",
        "if __name__ != \"__main__\":\n",
        "    from phomt_vocab import Vocab\n",
        "else:\n",
        "    # Thêm đường dẫn dự án để import được Vocab khi chạy file này độc lập\n",
        "    sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
        "    try:\n",
        "        from phomt_vocab import Vocab\n",
        "    except ImportError:\n",
        "        print(\"Không tìm thấy phomt_vocab.py. Đảm bảo nó cùng thư mục.\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "class phoMTDataset(Dataset):\n",
        "    def __init__(self, data_path, vocab):\n",
        "        self.vocab = vocab\n",
        "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        src_text = item[self.vocab.src_language]\n",
        "        tgt_text = item[self.vocab.tgt_language]\n",
        "\n",
        "        # Chuyển đổi thành indices\n",
        "        src_indices = self.vocab.encode(src_text, is_target=False)\n",
        "        tgt_indices = self.vocab.encode(tgt_text, is_target=True)\n",
        "\n",
        "        return {\n",
        "            'src': torch.tensor(src_indices, dtype=torch.long),\n",
        "            'tgt': torch.tensor(tgt_indices, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Tìm chiều dài lớn nhất của câu trong batch\n",
        "    src_lens = [len(item['src']) for item in batch]\n",
        "    tgt_lens = [len(item['tgt']) for item in batch]\n",
        "    max_src_len = max(src_lens)\n",
        "    max_tgt_len = max(tgt_lens)\n",
        "\n",
        "    # Lấy pad index (giả định pad_idx là 0)\n",
        "    pad_idx = 0\n",
        "\n",
        "    # Padding\n",
        "    padded_src = torch.full((len(batch), max_src_len), pad_idx, dtype=torch.long)\n",
        "    padded_tgt = torch.full((len(batch), max_tgt_len), pad_idx, dtype=torch.long)\n",
        "\n",
        "    for i, item in enumerate(batch):\n",
        "        padded_src[i, :src_lens[i]] = item['src']\n",
        "        padded_tgt[i, :tgt_lens[i]] = item['tgt']\n",
        "\n",
        "    return {\n",
        "        'src': padded_src,\n",
        "        'tgt': padded_tgt\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAPtXA2Tsdh-",
        "outputId": "3e939260-514e-4d7f-84cf-e60f1bfaf05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/seq2seq_bahdanau_lstm.py\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 1. ENCODER ---\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_encoder, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_encoder = n_encoder\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=d_model,\n",
        "            hidden_size=d_model,\n",
        "            num_layers=n_encoder,\n",
        "            dropout=dropout if n_encoder > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (batch_size, src_len)\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # output: (batch_size, src_len, d_model) -> H_s (cho Attention)\n",
        "        # hidden: (h_n, c_n): (n_encoder, batch_size, d_model) -> Initial Hidden cho Decoder\n",
        "        output, hidden = self.lstm(embedded)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "# --- 2. BAHDANAU ATTENTION MODULE ---\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # W_s * s_i (Trạng thái ẩn Decoder)\n",
        "        self.W_s = nn.Linear(d_model, d_model, bias=False)\n",
        "        # W_h * h_j (Các trạng thái ẩn Encoder)\n",
        "        self.W_h = nn.Linear(d_model, d_model, bias=False)\n",
        "        # V * tanh(.) (Vector ngữ cảnh)\n",
        "        self.V = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden_state, encoder_outputs):\n",
        "        # decoder_hidden_state (s_i): (batch_size, 1, d_model) (lớp trên cùng của h_n)\n",
        "        # encoder_outputs (h_j): (batch_size, src_len, d_model)\n",
        "\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        # 1. Mở rộng trạng thái decoder để khớp với chiều src_len\n",
        "        # s_i_expanded: (batch_size, src_len, d_model)\n",
        "        s_i_expanded = decoder_hidden_state.repeat(1, src_len, 1)\n",
        "\n",
        "        # 2. Tính Energy Scores (e_ij): V * tanh(W_s * s_i + W_h * h_j)\n",
        "        # energy: (batch_size, src_len, d_model)\n",
        "        energy = torch.tanh(self.W_s(s_i_expanded) + self.W_h(encoder_outputs))\n",
        "\n",
        "        # score: (batch_size, src_len, 1)\n",
        "        score = self.V(energy)\n",
        "\n",
        "        # 3. Tính Attention Weights (alpha_ij)\n",
        "        # attention_weights: (batch_size, src_len, 1) -> (batch_size, 1, src_len) sau transpose\n",
        "        attention_weights = F.softmax(score, dim=1).transpose(1, 2)\n",
        "\n",
        "        # 4. Tính Context Vector (c_i): c_i = sum(alpha_ij * h_j)\n",
        "        # context_vector: (batch_size, 1, d_model)\n",
        "        context_vector = torch.bmm(attention_weights, encoder_outputs)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# --- 3. DECODER VỚI BAHDANAU ATTENTION ---\n",
        "class AttentionDecoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_decoder, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_decoder = n_decoder\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "\n",
        "        # Decoder LSTM nhận đầu vào là [Embedded token || Context Vector]\n",
        "        # Kích thước đầu vào: d_model (Embedded) + d_model (Context) = 2 * d_model\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=2 * d_model,\n",
        "            hidden_size=d_model,\n",
        "            num_layers=n_decoder,\n",
        "            dropout=dropout if n_decoder > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "        self.attention = BahdanauAttention(d_model)\n",
        "\n",
        "        # Layer dự đoán cuối cùng: Input: d_model (h_n) -> Output: vocab_size\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # W_c để tính context vector cho bước 0 (sử dụng W_c * s_0)\n",
        "        self.W_c = nn.Linear(2 * d_model, d_model)\n",
        "\n",
        "    def forward_step(self, input_token, hidden, encoder_outputs):\n",
        "        # input_token: (batch_size, 1)\n",
        "        # hidden: (h_n, c_n) từ bước trước, mỗi tensor shape: (n_decoder, batch_size, d_model)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input_token))\n",
        "        # embedded: (batch_size, 1, d_model)\n",
        "\n",
        "        # Trạng thái ẩn của lớp trên cùng của LSTM (s_i) cho Bahdanau Attention\n",
        "        # decoder_hidden_state: (batch_size, 1, d_model)\n",
        "        # Lấy hidden state của lớp cuối cùng (n_decoder - 1)\n",
        "        decoder_hidden_state = hidden[0][-1, :, :].unsqueeze(1)\n",
        "\n",
        "        # 1. Attention (Tính Context Vector c_i)\n",
        "        # context: (batch_size, 1, d_model)\n",
        "        context, attn_weights = self.attention(decoder_hidden_state, encoder_outputs)\n",
        "\n",
        "        # 2. Concatenation (Embedded và Context)\n",
        "        # lstm_input: (batch_size, 1, 2*d_model)\n",
        "        lstm_input = torch.cat((embedded, context), dim=-1)\n",
        "\n",
        "        # 3. LSTM Step\n",
        "        # output: (batch_size, 1, d_model) -> H_t (Output của LSTM)\n",
        "        # hidden: (n_decoder, batch_size, d_model) -> State mới cho bước tiếp theo\n",
        "        output, hidden = self.lstm(lstm_input, hidden)\n",
        "\n",
        "        # 4. Final Prediction (Dự đoán từ)\n",
        "        # prediction: (batch_size, 1, vocab_size)\n",
        "        prediction = self.output_layer(output)\n",
        "\n",
        "        return prediction, hidden, attn_weights\n",
        "\n",
        "    def forward(self, tgt, initial_hidden, encoder_outputs):\n",
        "        # tgt: (batch_size, tgt_len) - Decoder input (<bos> y1 y2 ...)\n",
        "        batch_size, tgt_len = tgt.shape\n",
        "\n",
        "        all_predictions = torch.zeros(batch_size, tgt_len, self.vocab_size, device=tgt.device)\n",
        "        hidden = initial_hidden\n",
        "\n",
        "        for t in range(tgt_len):\n",
        "            input_token = tgt[:, t].unsqueeze(1)\n",
        "\n",
        "            prediction, hidden, _ = self.forward_step(input_token, hidden, encoder_outputs)\n",
        "\n",
        "            all_predictions[:, t] = prediction.squeeze(1)\n",
        "\n",
        "        return all_predictions, hidden\n",
        "\n",
        "\n",
        "class Seq2SeqBahdanauLSTM(nn.Module):\n",
        "    def __init__(self, d_model, n_encoder, n_decoder, dropout, vocab):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.encoder = EncoderLSTM(\n",
        "            vocab_size=vocab.src_vocab_size,\n",
        "            d_model=d_model,\n",
        "            n_encoder=n_encoder,\n",
        "            dropout=dropout,\n",
        "            pad_idx=vocab.pad_idx\n",
        "        )\n",
        "\n",
        "        self.decoder = AttentionDecoderLSTM(\n",
        "            vocab_size=vocab.tgt_vocab_size,\n",
        "            d_model=d_model,\n",
        "            n_decoder=n_decoder,\n",
        "            dropout=dropout,\n",
        "            pad_idx=vocab.pad_idx\n",
        "        )\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        encoder_outputs, encoder_hidden = self.encoder(src)\n",
        "        # output: (batch_size, tgt_len, vocab_size)\n",
        "        output, _ = self.decoder(tgt, encoder_hidden, encoder_outputs)\n",
        "\n",
        "        return output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, src, max_len=50):\n",
        "        self.eval()\n",
        "        batch_size = src.shape[0]\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        input_token = torch.full((batch_size, 1), self.vocab.bos_idx, dtype=torch.long, device=self.device)\n",
        "        output_tokens = torch.zeros((batch_size, max_len), dtype=torch.long, device=self.device)\n",
        "        output_tokens[:, 0] = self.vocab.bos_idx\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            prediction, hidden, _ = self.decoder.forward_step(input_token, hidden, encoder_outputs)\n",
        "\n",
        "            next_token = prediction.argmax(dim=-1)\n",
        "            output_tokens[:, t] = next_token.squeeze(-1)\n",
        "\n",
        "            if ((output_tokens[:, t] == self.vocab.eos_idx) | (output_tokens[:, t] == self.vocab.pad_idx)).all():\n",
        "                break\n",
        "\n",
        "            input_token = next_token\n",
        "\n",
        "        return output_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FReiSSKPr8If",
        "outputId": "fa535ba3-0cb5-4890-989f-2fba3fed9a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/seq2seq_bahdanau_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "adzO4eSOse6z",
        "outputId": "79d379d0-11c5-463d-f541-fc9f433593fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/train_bahdanau_lstm.py\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import các thành phần cơ bản\n",
        "from phomt_dataset import collate_fn, phoMTDataset\n",
        "from phomt_vocab import Vocab\n",
        "# ĐỔI: Import mô hình Bahdanau Attention mới\n",
        "from seq2seq_bahdanau_lstm import Seq2SeqBahdanauLSTM\n",
        "\n",
        "# Import Metrics\n",
        "try:\n",
        "    from torchmetrics.text.rouge import ROUGEScore\n",
        "except ImportError:\n",
        "    print(\"Vui lòng cài đặt torchmetrics: pip install torchmetrics\")\n",
        "    exit()\n",
        "\n",
        "# --- Config ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# CẬP NHẬT ĐƯỜNG DẪN CHECKPOINT MỚI\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/MODEL-BAHDANAU/\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/DATASET/small-PhoMT/\"\n",
        "\n",
        "\n",
        "# --- Logging Setup (Sử dụng format chỉ Message cho Console) ---\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "file_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "file_handler = logging.FileHandler(os.path.join(CHECKPOINT_DIR, \"training.log\"), mode='a')\n",
        "file_handler.setFormatter(file_formatter)\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "console_formatter = logging.Formatter(\"%(message)s\")\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setFormatter(console_formatter)\n",
        "logger.addHandler(console_handler)\n",
        "# --- Kết thúc Logging Setup ---\n",
        "\n",
        "def indices_to_text(indices, vocab, is_target=True):\n",
        "    tokens = []\n",
        "    i2s = vocab.tgt_i2s if is_target else vocab.src_i2s\n",
        "\n",
        "    for idx in indices:\n",
        "        if isinstance(idx, torch.Tensor):\n",
        "            idx = idx.item()\n",
        "\n",
        "        if is_target and idx == vocab.eos_idx:\n",
        "            break\n",
        "\n",
        "        if idx != vocab.pad_idx:\n",
        "            if is_target and idx == vocab.bos_idx:\n",
        "                continue\n",
        "\n",
        "            token = i2s.get(idx, vocab.unk_token)\n",
        "            tokens.append(token)\n",
        "\n",
        "        if not is_target and idx == vocab.eos_idx:\n",
        "            break\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Hàm train/evaluate không thay đổi logic, chỉ đổi mô hình\n",
        "def train(model: nn.Module, dataloader: DataLoader, epoch: int, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    running_loss = []\n",
        "\n",
        "    with tqdm(dataloader, desc=f\"Epoch {epoch} - Training\") as pbar:\n",
        "        for item in pbar:\n",
        "            src = item['src'].to(device)\n",
        "            tgt = item['tgt'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            decoder_input = tgt[:, :-1]\n",
        "            targets = tgt[:, 1:]\n",
        "\n",
        "            logits = model(src, decoder_input)\n",
        "\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), targets.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "            pbar.set_postfix({\"loss\": np.mean(running_loss)})\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = np.mean(running_loss)\n",
        "    logging.info(f\"--- Epoch {epoch} TRAIN finished --- Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate(model: nn.Module, dataloader: DataLoader, epoch: int, loss_fn, vocab):\n",
        "    model.eval()\n",
        "    running_loss = []\n",
        "    rouge_metric = ROUGEScore(rouge_keys=(\"rougeL\",)).to(device)\n",
        "    all_preds_text = []\n",
        "    all_targets_text = []\n",
        "\n",
        "    example_printed = False\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for item in tqdm(dataloader, desc=f\"Epoch {epoch} - Evaluating\"):\n",
        "            src = item['src'].to(device)\n",
        "            tgt = item['tgt'].to(device)\n",
        "\n",
        "            # 1. Validation Loss (Sử dụng Teacher Forcing)\n",
        "            decoder_input = tgt[:, :-1]\n",
        "            targets = tgt[:, 1:]\n",
        "            logits = model(src, decoder_input)\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), targets.reshape(-1))\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            # 2. ROUGE-L Prediction (Sử dụng Inference)\n",
        "            generated_tokens = model.predict(src, max_len=tgt.shape[1] + 10)\n",
        "\n",
        "            for i in range(len(tgt)):\n",
        "                pred_seq = generated_tokens[i].tolist()\n",
        "                pred_text = indices_to_text(pred_seq, vocab, is_target=True)\n",
        "\n",
        "                tgt_seq = tgt[i].tolist()\n",
        "                tgt_text = indices_to_text(tgt_seq, vocab, is_target=True)\n",
        "\n",
        "                # LOGIC IN VÍ DỤ\n",
        "                if not example_printed:\n",
        "                    src_seq = src[i].tolist()\n",
        "                    src_text = indices_to_text(src_seq, vocab, is_target=False)\n",
        "\n",
        "                    logging.info(f\"\\n======== Example Translation (Epoch {epoch}) ========\")\n",
        "                    logging.info(f\"-> Source (EN):     {src_text}\")\n",
        "                    logging.info(f\"-> Reference (VN):  {tgt_text}\")\n",
        "                    logging.info(f\"-> Prediction (VN): {pred_text}\")\n",
        "                    logging.info(\"==================================================\")\n",
        "                    example_printed = True\n",
        "\n",
        "                all_preds_text.append(pred_text)\n",
        "                all_targets_text.append(tgt_text)\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Tính ROUGE trên tập Validation\n",
        "    if len(all_preds_text) > 0:\n",
        "        rouge_scores = rouge_metric(all_preds_text, all_targets_text)\n",
        "        rouge_l = rouge_scores['rougeL_fmeasure'].item()\n",
        "    else:\n",
        "        rouge_l = 0.0\n",
        "\n",
        "    avg_loss = np.mean(running_loss)\n",
        "    logging.info(f\"--- Epoch {epoch} EVAL finished --- Val Loss: {avg_loss:.4f} | ROUGE-L: {rouge_l:.4f}\")\n",
        "\n",
        "    return avg_loss, rouge_l\n",
        "\n",
        "def visualize_metrics(train_losses, val_losses, rouge_scores):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
        "    plt.plot(epochs, val_losses, label='Val Loss', marker='s')\n",
        "    plt.title(\"Loss History\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, rouge_scores, label='Val ROUGE-L', marker='^', color='green')\n",
        "    plt.title(\"ROUGE-L Score History\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    logging.info(\"=\"*50)\n",
        "    logging.info(f\"Starting training Seq2Seq BAHDANAU ATTENTION LSTM on Device: {device}\")\n",
        "\n",
        "    vocab = Vocab(\n",
        "        path=DATASET_ROOT,\n",
        "        src_language=\"english\",\n",
        "        tgt_language=\"vietnamese\"\n",
        "    )\n",
        "\n",
        "    train_dataset = phoMTDataset(os.path.join(DATASET_ROOT, \"small-train.json\"), vocab)\n",
        "    dev_dataset = phoMTDataset(os.path.join(DATASET_ROOT, \"small-dev.json\"), vocab)\n",
        "    test_dataset = phoMTDataset(os.path.join(DATASET_ROOT, \"small-test.json\"), vocab)\n",
        "\n",
        "    logging.info(f\"Using full datasets: Train size={len(train_dataset)}, Dev size={len(dev_dataset)}, Test size={len(test_dataset)}\")\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # KHỞI TẠO MODEL BAHDANAU ATTENTION MỚI\n",
        "    model = Seq2SeqBahdanauLSTM(\n",
        "        d_model=256,\n",
        "        n_encoder=3,\n",
        "        n_decoder=3,\n",
        "        dropout=0.3,\n",
        "        vocab=vocab\n",
        "    ).to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    logging.info(f\"Model Parameters (LSTM with Bahdanau Attention): {total_params:,}\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)\n",
        "\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, \"best_bahdanau_lstm_mt.pt\")\n",
        "    best_rouge = 0.0\n",
        "    start_epoch = 0\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "            model.load_state_dict(ckpt['model_state_dict'])\n",
        "            optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "            start_epoch = ckpt['epoch']\n",
        "            best_rouge = ckpt.get('best_rouge', 0.0)\n",
        "            logging.info(f\"Resumed from epoch {start_epoch}, Best ROUGE: {best_rouge:.4f}\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n",
        "\n",
        "    train_losses, val_losses, val_rouges = [], [], []\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(start_epoch + 1, 20):\n",
        "        logging.info(f\"\\n--- Starting Epoch {epoch} ---\")\n",
        "\n",
        "        t_loss = train(model, train_loader, epoch, loss_fn, optimizer)\n",
        "        v_loss, v_rouge = evaluate(model, dev_loader, epoch, loss_fn, vocab)\n",
        "\n",
        "        train_losses.append(t_loss)\n",
        "        val_losses.append(v_loss)\n",
        "        val_rouges.append(v_rouge)\n",
        "\n",
        "        if v_rouge > best_rouge:\n",
        "            best_rouge = v_rouge\n",
        "            patience = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_rouge': best_rouge\n",
        "            }, checkpoint_path)\n",
        "            logging.info(f\"!!! NEW BEST MODEL (Epoch {epoch}) !!! Saved ROUGE-L: {best_rouge:.4f}\")\n",
        "        else:\n",
        "            patience += 1\n",
        "            logging.info(f\"No improvement. Patience: {patience}/10\")\n",
        "            if patience >= 10:\n",
        "                logging.info(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    logging.info(\"\\n================= Final Test Evaluation =================\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "\n",
        "        test_loss, test_rouge = evaluate(model, test_loader, 0, loss_fn, vocab)\n",
        "        logging.info(f\"Final Test Loss: {test_loss:.4f} | Test ROUGE-L: {test_rouge:.4f}\")\n",
        "    else:\n",
        "        logging.warning(\"Cannot evaluate on Test Set: Best model checkpoint not found.\")\n",
        "\n",
        "    visualize_metrics(train_losses, val_losses, val_rouges)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error: {str(e)}\", exc_info=True)\n",
        "        print(f\"\\n[FATAL ERROR] Check training.log for details. Error: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jIUtQH8sSPp",
        "outputId": "abe44e9e-0346-493c-b827-18d313aedc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/train_bahdanau_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/train_bahdanau_lstm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VzJiPorsytB",
        "outputId": "09b1cf8e-277a-475f-bdd8-774c290b6909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Starting training Seq2Seq BAHDANAU ATTENTION LSTM on Device: cuda\n",
            "Using full datasets: Train size=20000, Dev size=2000, Test size=2000\n",
            "Model Parameters (LSTM with Bahdanau Attention): 7,088,583\n",
            "\n",
            "--- Starting Epoch 1 ---\n",
            "Epoch 1 - Training: 100% 1250/1250 [04:49<00:00,  4.32it/s, loss=5.75]\n",
            "--- Epoch 1 TRAIN finished --- Loss: 5.7467\n",
            "Epoch 1 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 1) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): Và tôi có thể làm một người .\n",
            "==================================================\n",
            "Epoch 1 - Evaluating: 100% 125/125 [00:06<00:00, 19.43it/s]\n",
            "--- Epoch 1 EVAL finished --- Val Loss: 5.5881 | ROUGE-L: 0.2560\n",
            "!!! NEW BEST MODEL (Epoch 1) !!! Saved ROUGE-L: 0.2560\n",
            "\n",
            "--- Starting Epoch 2 ---\n",
            "Epoch 2 - Training: 100% 1250/1250 [04:44<00:00,  4.39it/s, loss=5.12]\n",
            "--- Epoch 2 TRAIN finished --- Loss: 5.1249\n",
            "Epoch 2 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 2) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): Chúng tôi có thể thấy một điều này , và tôi có thể thấy một điều này , và tôi có thể thấy một điều này , và chúng tôi có thể làm một điều này .\n",
            "==================================================\n",
            "Epoch 2 - Evaluating: 100% 125/125 [00:12<00:00, 10.17it/s]\n",
            "--- Epoch 2 EVAL finished --- Val Loss: 5.2674 | ROUGE-L: 0.2924\n",
            "!!! NEW BEST MODEL (Epoch 2) !!! Saved ROUGE-L: 0.2924\n",
            "\n",
            "--- Starting Epoch 3 ---\n",
            "Epoch 3 - Training: 100% 1250/1250 [04:46<00:00,  4.36it/s, loss=4.8]\n",
            "--- Epoch 3 TRAIN finished --- Loss: 4.8028\n",
            "Epoch 3 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 3) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> : Tôi có thể thấy một người khác , \" Tôi không thể làm gì , \" Tôi không thể làm gì ? \"\n",
            "==================================================\n",
            "Epoch 3 - Evaluating: 100% 125/125 [00:12<00:00, 10.22it/s]\n",
            "--- Epoch 3 EVAL finished --- Val Loss: 5.0394 | ROUGE-L: 0.2380\n",
            "No improvement. Patience: 1/10\n",
            "\n",
            "--- Starting Epoch 4 ---\n",
            "Epoch 4 - Training: 100% 1250/1250 [04:43<00:00,  4.41it/s, loss=4.54]\n",
            "--- Epoch 4 TRAIN finished --- Loss: 4.5396\n",
            "Epoch 4 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 4) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 4 - Evaluating: 100% 125/125 [00:12<00:00, 10.08it/s]\n",
            "--- Epoch 4 EVAL finished --- Val Loss: 4.8546 | ROUGE-L: 0.2306\n",
            "No improvement. Patience: 2/10\n",
            "\n",
            "--- Starting Epoch 5 ---\n",
            "Epoch 5 - Training: 100% 1250/1250 [04:40<00:00,  4.46it/s, loss=4.35]\n",
            "--- Epoch 5 TRAIN finished --- Loss: 4.3458\n",
            "Epoch 5 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 5) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 5 - Evaluating: 100% 125/125 [00:12<00:00, 10.32it/s]\n",
            "--- Epoch 5 EVAL finished --- Val Loss: 4.7370 | ROUGE-L: 0.2582\n",
            "No improvement. Patience: 3/10\n",
            "\n",
            "--- Starting Epoch 6 ---\n",
            "Epoch 6 - Training: 100% 1250/1250 [04:38<00:00,  4.48it/s, loss=4.2]\n",
            "--- Epoch 6 TRAIN finished --- Loss: 4.2001\n",
            "Epoch 6 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 6) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 6 - Evaluating: 100% 125/125 [00:12<00:00, 10.25it/s]\n",
            "--- Epoch 6 EVAL finished --- Val Loss: 4.6717 | ROUGE-L: 0.2630\n",
            "No improvement. Patience: 4/10\n",
            "\n",
            "--- Starting Epoch 7 ---\n",
            "Epoch 7 - Training: 100% 1250/1250 [04:39<00:00,  4.47it/s, loss=4.08]\n",
            "--- Epoch 7 TRAIN finished --- Loss: 4.0835\n",
            "Epoch 7 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 7) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 7 - Evaluating: 100% 125/125 [00:12<00:00, 10.27it/s]\n",
            "--- Epoch 7 EVAL finished --- Val Loss: 4.6070 | ROUGE-L: 0.2866\n",
            "No improvement. Patience: 5/10\n",
            "\n",
            "--- Starting Epoch 8 ---\n",
            "Epoch 8 - Training: 100% 1250/1250 [04:38<00:00,  4.48it/s, loss=3.98]\n",
            "--- Epoch 8 TRAIN finished --- Loss: 3.9846\n",
            "Epoch 8 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 8) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 8 - Evaluating: 100% 125/125 [00:12<00:00, 10.23it/s]\n",
            "--- Epoch 8 EVAL finished --- Val Loss: 4.5785 | ROUGE-L: 0.3013\n",
            "!!! NEW BEST MODEL (Epoch 8) !!! Saved ROUGE-L: 0.3013\n",
            "\n",
            "--- Starting Epoch 9 ---\n",
            "Epoch 9 - Training: 100% 1250/1250 [04:43<00:00,  4.42it/s, loss=3.9]\n",
            "--- Epoch 9 TRAIN finished --- Loss: 3.9013\n",
            "Epoch 9 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 9) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 9 - Evaluating: 100% 125/125 [00:12<00:00, 10.24it/s]\n",
            "--- Epoch 9 EVAL finished --- Val Loss: 4.5623 | ROUGE-L: 0.3036\n",
            "!!! NEW BEST MODEL (Epoch 9) !!! Saved ROUGE-L: 0.3036\n",
            "\n",
            "--- Starting Epoch 10 ---\n",
            "Epoch 10 - Training: 100% 1250/1250 [04:41<00:00,  4.44it/s, loss=3.83]\n",
            "--- Epoch 10 TRAIN finished --- Loss: 3.8294\n",
            "Epoch 10 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 10) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 10 - Evaluating: 100% 125/125 [00:12<00:00, 10.25it/s]\n",
            "--- Epoch 10 EVAL finished --- Val Loss: 4.5508 | ROUGE-L: 0.2965\n",
            "No improvement. Patience: 1/10\n",
            "\n",
            "--- Starting Epoch 11 ---\n",
            "Epoch 11 - Training: 100% 1250/1250 [04:39<00:00,  4.47it/s, loss=3.77]\n",
            "--- Epoch 11 TRAIN finished --- Loss: 3.7657\n",
            "Epoch 11 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 11) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 11 - Evaluating: 100% 125/125 [00:12<00:00, 10.15it/s]\n",
            "--- Epoch 11 EVAL finished --- Val Loss: 4.5420 | ROUGE-L: 0.3075\n",
            "!!! NEW BEST MODEL (Epoch 11) !!! Saved ROUGE-L: 0.3075\n",
            "\n",
            "--- Starting Epoch 12 ---\n",
            "Epoch 12 - Training: 100% 1250/1250 [04:39<00:00,  4.48it/s, loss=3.71]\n",
            "--- Epoch 12 TRAIN finished --- Loss: 3.7083\n",
            "Epoch 12 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 12) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 12 - Evaluating: 100% 125/125 [00:12<00:00, 10.21it/s]\n",
            "--- Epoch 12 EVAL finished --- Val Loss: 4.5376 | ROUGE-L: 0.3094\n",
            "!!! NEW BEST MODEL (Epoch 12) !!! Saved ROUGE-L: 0.3094\n",
            "\n",
            "--- Starting Epoch 13 ---\n",
            "Epoch 13 - Training: 100% 1250/1250 [04:41<00:00,  4.45it/s, loss=3.66]\n",
            "--- Epoch 13 TRAIN finished --- Loss: 3.6561\n",
            "Epoch 13 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 13) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 13 - Evaluating: 100% 125/125 [00:12<00:00, 10.18it/s]\n",
            "--- Epoch 13 EVAL finished --- Val Loss: 4.5483 | ROUGE-L: 0.2988\n",
            "No improvement. Patience: 1/10\n",
            "\n",
            "--- Starting Epoch 14 ---\n",
            "Epoch 14 - Training: 100% 1250/1250 [04:39<00:00,  4.48it/s, loss=3.61]\n",
            "--- Epoch 14 TRAIN finished --- Loss: 3.6101\n",
            "Epoch 14 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 14) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một nhà khoa học đã được chứng minh , và một số người có thể thấy được một chút , và một số người có thể thấy được từ những người khác , họ có thể thấy rằng các nhà khoa học đã được sử dụng .\n",
            "==================================================\n",
            "Epoch 14 - Evaluating: 100% 125/125 [00:12<00:00, 10.22it/s]\n",
            "--- Epoch 14 EVAL finished --- Val Loss: 4.5479 | ROUGE-L: 0.3160\n",
            "!!! NEW BEST MODEL (Epoch 14) !!! Saved ROUGE-L: 0.3160\n",
            "\n",
            "--- Starting Epoch 15 ---\n",
            "Epoch 15 - Training: 100% 1250/1250 [04:42<00:00,  4.43it/s, loss=3.57]\n",
            "--- Epoch 15 TRAIN finished --- Loss: 3.5658\n",
            "Epoch 15 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 15) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 15 - Evaluating: 100% 125/125 [00:12<00:00, 10.22it/s]\n",
            "--- Epoch 15 EVAL finished --- Val Loss: 4.5424 | ROUGE-L: 0.3244\n",
            "!!! NEW BEST MODEL (Epoch 15) !!! Saved ROUGE-L: 0.3244\n",
            "\n",
            "--- Starting Epoch 16 ---\n",
            "Epoch 16 - Training: 100% 1250/1250 [04:42<00:00,  4.42it/s, loss=3.53]\n",
            "--- Epoch 16 TRAIN finished --- Loss: 3.5250\n",
            "Epoch 16 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 16) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> <unk> <unk> , <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 16 - Evaluating: 100% 125/125 [00:12<00:00, 10.23it/s]\n",
            "--- Epoch 16 EVAL finished --- Val Loss: 4.5578 | ROUGE-L: 0.3188\n",
            "No improvement. Patience: 1/10\n",
            "\n",
            "--- Starting Epoch 17 ---\n",
            "Epoch 17 - Training: 100% 1250/1250 [04:38<00:00,  4.49it/s, loss=3.49]\n",
            "--- Epoch 17 TRAIN finished --- Loss: 3.4864\n",
            "Epoch 17 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 17) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 17 - Evaluating: 100% 125/125 [00:12<00:00, 10.21it/s]\n",
            "--- Epoch 17 EVAL finished --- Val Loss: 4.5594 | ROUGE-L: 0.3223\n",
            "No improvement. Patience: 2/10\n",
            "\n",
            "--- Starting Epoch 18 ---\n",
            "Epoch 18 - Training: 100% 1250/1250 [04:39<00:00,  4.47it/s, loss=3.45]\n",
            "--- Epoch 18 TRAIN finished --- Loss: 3.4499\n",
            "Epoch 18 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 18) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 18 - Evaluating: 100% 125/125 [00:12<00:00, 10.19it/s]\n",
            "--- Epoch 18 EVAL finished --- Val Loss: 4.5739 | ROUGE-L: 0.3206\n",
            "No improvement. Patience: 3/10\n",
            "\n",
            "--- Starting Epoch 19 ---\n",
            "Epoch 19 - Training: 100% 1250/1250 [04:39<00:00,  4.47it/s, loss=3.42]\n",
            "--- Epoch 19 TRAIN finished --- Loss: 3.4155\n",
            "Epoch 19 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 19) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 19 - Evaluating: 100% 125/125 [00:12<00:00,  9.93it/s]\n",
            "--- Epoch 19 EVAL finished --- Val Loss: 4.5857 | ROUGE-L: 0.3253\n",
            "!!! NEW BEST MODEL (Epoch 19) !!! Saved ROUGE-L: 0.3253\n",
            "\n",
            "================= Final Test Evaluation =================\n",
            "Epoch 0 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 0) ========\n",
            "-> Source (EN):     Brother Albert <unk> and his wife , <unk> Susan <unk> , from the West <unk> in <unk> , <unk>\n",
            "-> Reference (VN):  Anh Albert <unk> và chị Susan <unk> , thuộc hội thánh West ở <unk> , <unk>\n",
            "-> Prediction (VN): <unk> <unk> <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 0 - Evaluating: 100% 125/125 [00:16<00:00,  7.75it/s]\n",
            "--- Epoch 0 EVAL finished --- Val Loss: 4.4299 | ROUGE-L: 0.3324\n",
            "Final Test Loss: 4.4299 | Test ROUGE-L: 0.3324\n",
            "Figure(1500x600)\n"
          ]
        }
      ]
    }
  ]
}