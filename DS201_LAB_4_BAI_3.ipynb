{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGw-XzywRZ0N",
        "outputId": "fe61f73c-4308-48b6-c8db-31fe862d8584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_vocab.py\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, path, src_language, tgt_language, min_freq=5):\n",
        "        self.src_language = src_language\n",
        "        self.tgt_language = tgt_language\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.unk_token = \"<unk>\"\n",
        "        self.bos_token = \"<bos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "\n",
        "        self.pad_idx = 0\n",
        "        self.unk_idx = 1\n",
        "        self.bos_idx = 2\n",
        "        self.eos_idx = 3\n",
        "\n",
        "        self.src_s2i = {self.pad_token: self.pad_idx, self.unk_token: self.unk_idx}\n",
        "        self.src_i2s = {self.pad_idx: self.pad_token, self.unk_idx: self.unk_token}\n",
        "        self.tgt_s2i = {self.pad_token: self.pad_idx, self.unk_token: self.unk_idx, self.bos_token: self.bos_idx, self.eos_token: self.eos_idx}\n",
        "        self.tgt_i2s = {self.pad_idx: self.pad_token, self.unk_idx: self.unk_token, self.bos_idx: self.bos_token, self.eos_idx: self.eos_token}\n",
        "\n",
        "        self.build_vocab(path)\n",
        "\n",
        "    def load_data(self, path):\n",
        "        files = [\"small-train.json\", \"small-dev.json\", \"small-test.json\"]\n",
        "        data = []\n",
        "        for file in files:\n",
        "            full_path = os.path.join(path, file)\n",
        "            if os.path.exists(full_path):\n",
        "                with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    data.extend(json.load(f))\n",
        "        return data\n",
        "\n",
        "    def build_vocab(self, path):\n",
        "        data = self.load_data(path)\n",
        "\n",
        "        src_tokens = [item[self.src_language].split() for item in data]\n",
        "        tgt_tokens = [item[self.tgt_language].split() for item in data]\n",
        "\n",
        "        src_counter = Counter(chain.from_iterable(src_tokens))\n",
        "        tgt_counter = Counter(chain.from_iterable(tgt_tokens))\n",
        "\n",
        "        # Xây dựng từ điển Source (tiếng Anh)\n",
        "        for token, count in src_counter.items():\n",
        "            if count >= self.min_freq and token not in self.src_s2i:\n",
        "                idx = len(self.src_s2i)\n",
        "                self.src_s2i[token] = idx\n",
        "                self.src_i2s[idx] = token\n",
        "\n",
        "        # Xây dựng từ điển Target (tiếng Việt)\n",
        "        for token, count in tgt_counter.items():\n",
        "            if count >= self.min_freq and token not in self.tgt_s2i:\n",
        "                idx = len(self.tgt_s2i)\n",
        "                self.tgt_s2i[token] = idx\n",
        "                self.tgt_i2s[idx] = token\n",
        "\n",
        "        self.src_vocab_size = len(self.src_s2i)\n",
        "        self.tgt_vocab_size = len(self.tgt_s2i)\n",
        "\n",
        "    def encode(self, text, is_target=False):\n",
        "        tokens = text.split()\n",
        "        s2i = self.tgt_s2i if is_target else self.src_s2i\n",
        "        unk_idx = self.unk_idx\n",
        "\n",
        "        indices = [s2i.get(token, unk_idx) for token in tokens]\n",
        "\n",
        "        if is_target:\n",
        "            indices = [self.bos_idx] + indices + [self.eos_idx]\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def decode(self, indices, is_target=False):\n",
        "        i2s = self.tgt_i2s if is_target else self.src_i2s\n",
        "        tokens = [i2s.get(idx, self.unk_token) for idx in indices]\n",
        "        return \" \".join(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFQWWo9-r5LF",
        "outputId": "1dd92acf-d77f-4624-81f4-4840a12fe607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_vocab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_dataset.py\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Đảm bảo có thể import Vocab nếu file này được chạy độc lập\n",
        "if __name__ != \"__main__\":\n",
        "    from phomt_vocab import Vocab\n",
        "else:\n",
        "    # Thêm đường dẫn dự án để import được Vocab khi chạy file này độc lập\n",
        "    sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
        "    try:\n",
        "        from phomt_vocab import Vocab\n",
        "    except ImportError:\n",
        "        print(\"Không tìm thấy phomt_vocab.py. Đảm bảo nó cùng thư mục.\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "class phoMTDataset(Dataset):\n",
        "    def __init__(self, data_path, vocab):\n",
        "        self.vocab = vocab\n",
        "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        src_text = item[self.vocab.src_language]\n",
        "        tgt_text = item[self.vocab.tgt_language]\n",
        "\n",
        "        # Chuyển đổi thành indices\n",
        "        src_indices = self.vocab.encode(src_text, is_target=False)\n",
        "        tgt_indices = self.vocab.encode(tgt_text, is_target=True)\n",
        "\n",
        "        return {\n",
        "            'src': torch.tensor(src_indices, dtype=torch.long),\n",
        "            'tgt': torch.tensor(tgt_indices, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Tìm chiều dài lớn nhất của câu trong batch\n",
        "    src_lens = [len(item['src']) for item in batch]\n",
        "    tgt_lens = [len(item['tgt']) for item in batch]\n",
        "    max_src_len = max(src_lens)\n",
        "    max_tgt_len = max(tgt_lens)\n",
        "\n",
        "    # Lấy pad index (giả định pad_idx là 0)\n",
        "    pad_idx = 0\n",
        "\n",
        "    # Padding\n",
        "    padded_src = torch.full((len(batch), max_src_len), pad_idx, dtype=torch.long)\n",
        "    padded_tgt = torch.full((len(batch), max_tgt_len), pad_idx, dtype=torch.long)\n",
        "\n",
        "    for i, item in enumerate(batch):\n",
        "        padded_src[i, :src_lens[i]] = item['src']\n",
        "        padded_tgt[i, :tgt_lens[i]] = item['tgt']\n",
        "\n",
        "    return {\n",
        "        'src': padded_src,\n",
        "        'tgt': padded_tgt\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAPtXA2Tsdh-",
        "outputId": "3e939260-514e-4d7f-84cf-e60f1bfaf05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/phomt_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-6BJVMhpxofQ",
        "outputId": "01759213-cd6c-4117-b4ce-34df8093632f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/seq2seq_luong_lstm.py\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 1. ENCODER ---\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_encoder, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_encoder = n_encoder\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "\n",
        "        # LSTM Unidirectional\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=d_model,\n",
        "            hidden_size=d_model,\n",
        "            num_layers=n_encoder,\n",
        "            dropout=dropout if n_encoder > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (batch_size, src_len)\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded: (batch_size, src_len, d_model)\n",
        "\n",
        "        # output: (batch_size, src_len, d_model) -> output này chính là H_s dùng trong Attention\n",
        "        # hidden: (h_n, c_n): (n_encoder, batch_size, d_model)\n",
        "        output, hidden = self.lstm(embedded)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "# --- 2. LUONG ATTENTION MODULE ---\n",
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        # Luong Attention sử dụng cơ chế \"Dot Product\" nếu d_model bằng nhau\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, decoder_output, encoder_outputs):\n",
        "        # decoder_output (Q): H_t (trạng thái ẩn hiện tại của decoder) - (batch_size, 1, d_model)\n",
        "        # encoder_outputs (K): H_s (các trạng thái ẩn của encoder) - (batch_size, src_len, d_model)\n",
        "\n",
        "        # Tính điểm năng lượng (energy scores): score(H_t, H_s)\n",
        "        # Luong 'dot' score: H_t * H_s^T\n",
        "        # scores: (batch_size, 1, src_len)\n",
        "        scores = torch.bmm(decoder_output, encoder_outputs.transpose(1, 2))\n",
        "\n",
        "        # Tính trọng số Attention: attention_weights\n",
        "        # attention_weights: (batch_size, 1, src_len) -> tổng theo chiều src_len = 1\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Tính Context Vector (C_t): Context = attention_weights * H_s\n",
        "        # context_vector: (batch_size, 1, d_model)\n",
        "        context_vector = torch.bmm(attention_weights, encoder_outputs)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# --- 3. DECODER VỚI ATTENTION ---\n",
        "class AttentionDecoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_decoder, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_decoder = n_decoder\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "\n",
        "        # LSTM nhận đầu vào là (embedded + context) nhưng ở đây ta dùng embedded (d_model)\n",
        "        # và kết hợp context sau bước LSTM (Luông)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=d_model,\n",
        "            hidden_size=d_model,\n",
        "            num_layers=n_decoder,\n",
        "            dropout=dropout if n_decoder > 1 else 0,\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "        self.attention = LuongAttention(d_model)\n",
        "\n",
        "        # Layer kết hợp output của LSTM và Context vector\n",
        "        # Input size: 2 * d_model (h_t + C_t)\n",
        "        self.concat_layer = nn.Linear(2 * d_model, d_model)\n",
        "\n",
        "        # Layer dự đoán cuối cùng\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward_step(self, input_token, hidden, encoder_outputs):\n",
        "        # input_token: (batch_size, 1)\n",
        "        # hidden: (h_n, c_n) từ bước trước, mỗi tensor shape: (n_decoder, batch_size, d_model)\n",
        "        # encoder_outputs: (batch_size, src_len, d_model)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input_token))\n",
        "        # embedded: (batch_size, 1, d_model)\n",
        "\n",
        "        # 1. LSTM Step\n",
        "        # output: (batch_size, 1, d_model) -> H_t\n",
        "        # hidden: (n_decoder, batch_size, d_model) -> H_n, C_n cho bước tiếp theo\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        # 2. Attention\n",
        "        # context: (batch_size, 1, d_model)\n",
        "        # attn_weights: (batch_size, 1, src_len)\n",
        "        context, attn_weights = self.attention(output, encoder_outputs)\n",
        "\n",
        "        # 3. Concatenation (H_t và C_t) và Projection\n",
        "        # Concatenated: (batch_size, 1, 2*d_model)\n",
        "        concat_input = torch.cat((output, context), dim=-1)\n",
        "\n",
        "        # Concat -> Tanh -> Projected: (batch_size, 1, d_model)\n",
        "        output_att = torch.tanh(self.concat_layer(concat_input))\n",
        "\n",
        "        # 4. Final Prediction Layer\n",
        "        # prediction: (batch_size, 1, vocab_size)\n",
        "        prediction = self.output_layer(output_att)\n",
        "\n",
        "        return prediction, hidden, attn_weights\n",
        "\n",
        "    def forward(self, tgt, initial_hidden, encoder_outputs):\n",
        "        # tgt: (batch_size, tgt_len) - Decoder input (<bos> y1 y2 ...)\n",
        "        batch_size, tgt_len = tgt.shape\n",
        "\n",
        "        # tensor lưu trữ logits dự đoán\n",
        "        all_predictions = torch.zeros(batch_size, tgt_len, self.vocab_size, device=tgt.device)\n",
        "\n",
        "        # Hidden state ban đầu lấy từ Encoder\n",
        "        hidden = initial_hidden\n",
        "\n",
        "        # Chạy Decoder theo từng bước thời gian (teacher forcing)\n",
        "        for t in range(tgt_len):\n",
        "            # input_token: (batch_size, 1)\n",
        "            input_token = tgt[:, t].unsqueeze(1)\n",
        "\n",
        "            # prediction: (batch_size, 1, vocab_size)\n",
        "            prediction, hidden, _ = self.forward_step(input_token, hidden, encoder_outputs)\n",
        "\n",
        "            # Lưu trữ prediction\n",
        "            all_predictions[:, t] = prediction.squeeze(1)\n",
        "\n",
        "        return all_predictions, hidden\n",
        "\n",
        "\n",
        "class Seq2SeqAttentionLSTM(nn.Module):\n",
        "    def __init__(self, d_model, n_encoder, n_decoder, dropout, vocab):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Encoder (không đổi)\n",
        "        self.encoder = EncoderLSTM(\n",
        "            vocab_size=vocab.src_vocab_size,\n",
        "            d_model=d_model,\n",
        "            n_encoder=n_encoder,\n",
        "            dropout=dropout,\n",
        "            pad_idx=vocab.pad_idx\n",
        "        )\n",
        "\n",
        "        # Decoder (Attention)\n",
        "        self.decoder = AttentionDecoderLSTM(\n",
        "            vocab_size=vocab.tgt_vocab_size,\n",
        "            d_model=d_model,\n",
        "            n_decoder=n_decoder,\n",
        "            dropout=dropout,\n",
        "            pad_idx=vocab.pad_idx\n",
        "        )\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # src: (batch_size, src_len)\n",
        "        # tgt: (batch_size, tgt_len)\n",
        "\n",
        "        # encoder_outputs: (batch_size, src_len, d_model) - Dùng cho Attention\n",
        "        # encoder_hidden: (h_n, c_n): (n_layers, batch_size, d_model) - Dùng làm initial hidden\n",
        "        encoder_outputs, encoder_hidden = self.encoder(src)\n",
        "\n",
        "        # output: (batch_size, tgt_len, vocab_size)\n",
        "        output, _ = self.decoder(tgt, encoder_hidden, encoder_outputs)\n",
        "\n",
        "        return output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, src, max_len=50):\n",
        "        self.eval()\n",
        "        batch_size = src.shape[0]\n",
        "\n",
        "        # 1. Encoding\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # 2. Decoding - Bắt đầu với <bos> token\n",
        "        input_token = torch.full((batch_size, 1), self.vocab.bos_idx, dtype=torch.long, device=self.device)\n",
        "\n",
        "        output_tokens = torch.zeros((batch_size, max_len), dtype=torch.long, device=self.device)\n",
        "        output_tokens[:, 0] = self.vocab.bos_idx\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            # prediction: (batch_size, 1, vocab_size)\n",
        "            # hidden: state mới\n",
        "            prediction, hidden, _ = self.decoder.forward_step(input_token, hidden, encoder_outputs)\n",
        "\n",
        "            # Lấy token có xác suất cao nhất: next_token (batch_size, 1)\n",
        "            next_token = prediction.argmax(dim=-1)\n",
        "\n",
        "            # Lưu token vào tensor kết quả\n",
        "            output_tokens[:, t] = next_token.squeeze(-1)\n",
        "\n",
        "            # Dừng nếu tất cả các câu trong batch đã sinh ra <eos>\n",
        "            if ((output_tokens[:, t] == self.vocab.eos_idx) | (output_tokens[:, t] == self.vocab.pad_idx)).all():\n",
        "                break\n",
        "\n",
        "            # Cập nhật input cho bước thời gian tiếp theo\n",
        "            input_token = next_token # (batch_size, 1)\n",
        "\n",
        "        return output_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWqtwcHy5wbE",
        "outputId": "72ab9faf-947f-48f7-a129-ba99791a654c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/seq2seq_luong_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/train_luong_lstm.py\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import các thành phần cơ bản\n",
        "from phomt_dataset import collate_fn, phoMTDataset\n",
        "from phomt_vocab import Vocab\n",
        "# ĐỔI: Import mô hình Attention mới\n",
        "from seq2seq_attention_lstm import Seq2SeqAttentionLSTM\n",
        "\n",
        "# Import Metrics\n",
        "try:\n",
        "    from torchmetrics.text.rouge import ROUGEScore\n",
        "except ImportError:\n",
        "    print(\"Vui lòng cài đặt torchmetrics: pip install torchmetrics\")\n",
        "    exit()\n",
        "\n",
        "# --- Config ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# CẬP NHẬT ĐƯỜNG DẪN CHECKPOINT MỚI\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/MODEL-LUONG/\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/DATASET/small-PhoMT/\"\n",
        "\n",
        "\n",
        "# --- Logging Setup (Sử dụng format chỉ Message cho Console) ---\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "file_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "file_handler = logging.FileHandler(os.path.join(CHECKPOINT_DIR, \"training.log\"), mode='a')\n",
        "file_handler.setFormatter(file_formatter)\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "console_formatter = logging.Formatter(\"%(message)s\")\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setFormatter(console_formatter)\n",
        "logger.addHandler(console_handler)\n",
        "# --- Kết thúc Logging Setup ---\n",
        "\n",
        "def indices_to_text(indices, vocab, is_target=True):\n",
        "    tokens = []\n",
        "    i2s = vocab.tgt_i2s if is_target else vocab.src_i2s\n",
        "\n",
        "    for idx in indices:\n",
        "        if isinstance(idx, torch.Tensor):\n",
        "            idx = idx.item()\n",
        "\n",
        "        if is_target and idx == vocab.eos_idx:\n",
        "            break\n",
        "\n",
        "        if idx != vocab.pad_idx:\n",
        "            if is_target and idx == vocab.bos_idx:\n",
        "                continue\n",
        "\n",
        "            token = i2s.get(idx, vocab.unk_token)\n",
        "            tokens.append(token)\n",
        "\n",
        "        if not is_target and idx == vocab.eos_idx:\n",
        "            break\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Hàm train/evaluate không thay đổi logic, chỉ đổi mô hình\n",
        "def train(model: nn.Module, dataloader: DataLoader, epoch: int, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    running_loss = []\n",
        "\n",
        "    with tqdm(dataloader, desc=f\"Epoch {epoch} - Training\") as pbar:\n",
        "        for item in pbar:\n",
        "            src = item['src'].to(device)\n",
        "            tgt = item['tgt'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            decoder_input = tgt[:, :-1]\n",
        "            targets = tgt[:, 1:]\n",
        "\n",
        "            logits = model(src, decoder_input)\n",
        "\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), targets.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "            pbar.set_postfix({\"loss\": np.mean(running_loss)})\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = np.mean(running_loss)\n",
        "    logging.info(f\"--- Epoch {epoch} TRAIN finished --- Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate(model: nn.Module, dataloader: DataLoader, epoch: int, loss_fn, vocab):\n",
        "    model.eval()\n",
        "    running_loss = []\n",
        "    rouge_metric = ROUGEScore(rouge_keys=(\"rougeL\",)).to(device)\n",
        "    all_preds_text = []\n",
        "    all_targets_text = []\n",
        "\n",
        "    example_printed = False\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for item in tqdm(dataloader, desc=f\"Epoch {epoch} - Evaluating\"):\n",
        "            src = item['src'].to(device)\n",
        "            tgt = item['tgt'].to(device)\n",
        "\n",
        "            # 1. Validation Loss (Sử dụng Teacher Forcing)\n",
        "            decoder_input = tgt[:, :-1]\n",
        "            targets = tgt[:, 1:]\n",
        "            logits = model(src, decoder_input)\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), targets.reshape(-1))\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            # 2. ROUGE-L Prediction (Sử dụng Inference)\n",
        "            generated_tokens = model.predict(src, max_len=tgt.shape[1] + 10)\n",
        "\n",
        "            for i in range(len(tgt)):\n",
        "                pred_seq = generated_tokens[i].tolist()\n",
        "                pred_text = indices_to_text(pred_seq, vocab, is_target=True)\n",
        "\n",
        "                tgt_seq = tgt[i].tolist()\n",
        "                tgt_text = indices_to_text(tgt_seq, vocab, is_target=True)\n",
        "\n",
        "                # LOGIC IN VÍ DỤ\n",
        "                if not example_printed:\n",
        "                    src_seq = src[i].tolist()\n",
        "                    src_text = indices_to_text(src_seq, vocab, is_target=False)\n",
        "\n",
        "                    logging.info(f\"\\n======== Example Translation (Epoch {epoch}) ========\")\n",
        "                    logging.info(f\"-> Source (EN):     {src_text}\")\n",
        "                    logging.info(f\"-> Reference (VN):  {tgt_text}\")\n",
        "                    logging.info(f\"-> Prediction (VN): {pred_text}\")\n",
        "                    logging.info(\"==================================================\")\n",
        "                    example_printed = True\n",
        "\n",
        "                all_preds_text.append(pred_text)\n",
        "                all_targets_text.append(tgt_text)\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Tính ROUGE trên tập Validation\n",
        "    if len(all_preds_text) > 0:\n",
        "        rouge_scores = rouge_metric(all_preds_text, all_targets_text)\n",
        "        rouge_l = rouge_scores['rougeL_fmeasure'].item()\n",
        "    else:\n",
        "        rouge_l = 0.0\n",
        "\n",
        "    avg_loss = np.mean(running_loss)\n",
        "    logging.info(f\"--- Epoch {epoch} EVAL finished --- Val Loss: {avg_loss:.4f} | ROUGE-L: {rouge_l:.4f}\")\n",
        "\n",
        "    return avg_loss, rouge_l\n",
        "\n",
        "def visualize_metrics(train_losses, val_losses, rouge_scores):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
        "    plt.plot(epochs, val_losses, label='Val Loss', marker='s')\n",
        "    plt.title(\"Loss History\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, rouge_scores, label='Val ROUGE-L', marker='^', color='green')\n",
        "    plt.title(\"ROUGE-L Score History\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    logging.info(\"=\"*50)\n",
        "    logging.info(f\"Starting training Seq2Seq LUONG ATTENTION LSTM on Device: {device}\")\n",
        "\n",
        "    vocab = Vocab(\n",
        "        path=DATASET_ROOT,\n",
        "        src_language=\"english\",\n",
        "        tgt_language=\"vietnamese\"\n",
        "    )\n",
        "\n",
        "    train_dataset = phoMTDataset(os.path.join(DATASET_ROOT, \"small-train.json\"), vocab)\n",
        "    dev_dataset = phoMTDataset(os.path.join(DATASET_ROOT, \"small-dev.json\"), vocab)\n",
        "    test_dataset = phoMTDataset(os.path.join(DATASET_ROOT, \"small-test.json\"), vocab)\n",
        "\n",
        "    logging.info(f\"Using full datasets: Train size={len(train_dataset)}, Dev size={len(dev_dataset)}, Test size={len(test_dataset)}\")\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # KHỞI TẠO MODEL ATTENTION MỚI\n",
        "    model = Seq2SeqAttentionLSTM(\n",
        "        d_model=256,\n",
        "        n_encoder=3,\n",
        "        n_decoder=3,\n",
        "        dropout=0.3,\n",
        "        vocab=vocab\n",
        "    ).to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    logging.info(f\"Model Parameters (LSTM with Luong Attention): {total_params:,}\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)\n",
        "\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, \"best_luong_lstm_mt.pt\")\n",
        "    best_rouge = 0.0\n",
        "    start_epoch = 0\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "            model.load_state_dict(ckpt['model_state_dict'])\n",
        "            optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "            start_epoch = ckpt['epoch']\n",
        "            best_rouge = ckpt.get('best_rouge', 0.0)\n",
        "            logging.info(f\"Resumed from epoch {start_epoch}, Best ROUGE: {best_rouge:.4f}\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n",
        "\n",
        "    train_losses, val_losses, val_rouges = [], [], []\n",
        "    patience = 0\n",
        "\n",
        "    for epoch in range(start_epoch + 1, 20):\n",
        "        logging.info(f\"\\n--- Starting Epoch {epoch} ---\")\n",
        "\n",
        "        t_loss = train(model, train_loader, epoch, loss_fn, optimizer)\n",
        "        v_loss, v_rouge = evaluate(model, dev_loader, epoch, loss_fn, vocab)\n",
        "\n",
        "        train_losses.append(t_loss)\n",
        "        val_losses.append(v_loss)\n",
        "        val_rouges.append(v_rouge)\n",
        "\n",
        "        if v_rouge > best_rouge:\n",
        "            best_rouge = v_rouge\n",
        "            patience = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_rouge': best_rouge\n",
        "            }, checkpoint_path)\n",
        "            logging.info(f\"!!! NEW BEST MODEL (Epoch {epoch}) !!! Saved ROUGE-L: {best_rouge:.4f}\")\n",
        "        else:\n",
        "            patience += 1\n",
        "            logging.info(f\"No improvement. Patience: {patience}/10\")\n",
        "            if patience >= 10:\n",
        "                logging.info(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    logging.info(\"\\n================= Final Test Evaluation =================\")\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "\n",
        "        test_loss, test_rouge = evaluate(model, test_loader, 0, loss_fn, vocab)\n",
        "        logging.info(f\"Final Test Loss: {test_loss:.4f} | Test ROUGE-L: {test_rouge:.4f}\")\n",
        "    else:\n",
        "        logging.warning(\"Cannot evaluate on Test Set: Best model checkpoint not found.\")\n",
        "\n",
        "    visualize_metrics(train_losses, val_losses, val_rouges)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error: {str(e)}\", exc_info=True)\n",
        "        print(f\"\\n[FATAL ERROR] Check training.log for details. Error: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkh6qDyE51Ux",
        "outputId": "cc7de8c7-a333-4a26-9d8b-87a92921c0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/train_luong_lstm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/DATA-SCIENCE-SUBJECT/DS201/PRACTICE/DS201-LAB4/train_luong_lstm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4e1MeYp56GY",
        "outputId": "bfba2af4-a551-4143-c7a6-ef44e1928e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Starting training Seq2Seq LUONG ATTENTION LSTM on Device: cuda\n",
            "Using full datasets: Train size=20000, Dev size=2000, Test size=2000\n",
            "Model Parameters (LSTM with Luong Attention): 6,695,111\n",
            "\n",
            "--- Starting Epoch 1 ---\n",
            "Epoch 1 - Training: 100% 1250/1250 [03:57<00:00,  5.27it/s, loss=5.63]\n",
            "--- Epoch 1 TRAIN finished --- Loss: 5.6264\n",
            "Epoch 1 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 1) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "==================================================\n",
            "Epoch 1 - Evaluating: 100% 125/125 [00:10<00:00, 12.05it/s]\n",
            "--- Epoch 1 EVAL finished --- Val Loss: 5.3477 | ROUGE-L: 0.2097\n",
            "!!! NEW BEST MODEL (Epoch 1) !!! Saved ROUGE-L: 0.2097\n",
            "\n",
            "--- Starting Epoch 2 ---\n",
            "Epoch 2 - Training: 100% 1250/1250 [03:55<00:00,  5.30it/s, loss=4.77]\n",
            "--- Epoch 2 TRAIN finished --- Loss: 4.7749\n",
            "Epoch 2 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 2) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , <unk> <unk> , <unk> <unk> <unk> <unk> , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> .\n",
            "==================================================\n",
            "Epoch 2 - Evaluating: 100% 125/125 [00:09<00:00, 12.83it/s]\n",
            "--- Epoch 2 EVAL finished --- Val Loss: 4.8356 | ROUGE-L: 0.3038\n",
            "!!! NEW BEST MODEL (Epoch 2) !!! Saved ROUGE-L: 0.3038\n",
            "\n",
            "--- Starting Epoch 3 ---\n",
            "Epoch 3 - Training: 100% 1250/1250 [03:56<00:00,  5.27it/s, loss=4.3]\n",
            "--- Epoch 3 TRAIN finished --- Loss: 4.2956\n",
            "Epoch 3 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 3) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những người khác trên thế giới , một người khác trên thế giới , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 3 - Evaluating: 100% 125/125 [00:09<00:00, 13.23it/s]\n",
            "--- Epoch 3 EVAL finished --- Val Loss: 4.5211 | ROUGE-L: 0.3365\n",
            "!!! NEW BEST MODEL (Epoch 3) !!! Saved ROUGE-L: 0.3365\n",
            "\n",
            "--- Starting Epoch 4 ---\n",
            "Epoch 4 - Training: 100% 1250/1250 [03:56<00:00,  5.29it/s, loss=3.97]\n",
            "--- Epoch 4 TRAIN finished --- Loss: 3.9682\n",
            "Epoch 4 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 4) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những điều khác biệt nhất trong vòng 40 năm , làm việc với một người phụ nữ trong vòng <unk> <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 4 - Evaluating: 100% 125/125 [00:09<00:00, 12.62it/s]\n",
            "--- Epoch 4 EVAL finished --- Val Loss: 4.3073 | ROUGE-L: 0.3508\n",
            "!!! NEW BEST MODEL (Epoch 4) !!! Saved ROUGE-L: 0.3508\n",
            "\n",
            "--- Starting Epoch 5 ---\n",
            "Epoch 5 - Training: 100% 1250/1250 [03:57<00:00,  5.26it/s, loss=3.73]\n",
            "--- Epoch 5 TRAIN finished --- Loss: 3.7326\n",
            "Epoch 5 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 5) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những vấn đề lớn nhất đã xảy ra ở Mỹ , làm việc có thể được kiểm soát bởi một chiếc xe đạp <unk> ở Bắc Kinh , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 5 - Evaluating: 100% 125/125 [00:10<00:00, 12.31it/s]\n",
            "--- Epoch 5 EVAL finished --- Val Loss: 4.1699 | ROUGE-L: 0.3664\n",
            "!!! NEW BEST MODEL (Epoch 5) !!! Saved ROUGE-L: 0.3664\n",
            "\n",
            "--- Starting Epoch 6 ---\n",
            "Epoch 6 - Training: 100% 1250/1250 [03:55<00:00,  5.30it/s, loss=3.55]\n",
            "--- Epoch 6 TRAIN finished --- Loss: 3.5481\n",
            "Epoch 6 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 6) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những công nghệ lớn nhất đang diễn ra trong lịch sử của Ấn Độ , làm <unk> <unk> <unk> <unk> trong những năm <unk> trong những năm <unk> trong những năm 1980 , <unk> .\n",
            "==================================================\n",
            "Epoch 6 - Evaluating: 100% 125/125 [00:10<00:00, 12.44it/s]\n",
            "--- Epoch 6 EVAL finished --- Val Loss: 4.0726 | ROUGE-L: 0.3781\n",
            "!!! NEW BEST MODEL (Epoch 6) !!! Saved ROUGE-L: 0.3781\n",
            "\n",
            "--- Starting Epoch 7 ---\n",
            "Epoch 7 - Training: 100% 1250/1250 [03:56<00:00,  5.28it/s, loss=3.4]\n",
            "--- Epoch 7 TRAIN finished --- Loss: 3.3983\n",
            "Epoch 7 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 7) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những điều quan trọng nhất đã diễn ra trong lịch sử Mỹ , làm việc <unk> <unk> <unk> <unk> <unk> <unk> ở Đức , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 7 - Evaluating: 100% 125/125 [00:10<00:00, 12.26it/s]\n",
            "--- Epoch 7 EVAL finished --- Val Loss: 4.0105 | ROUGE-L: 0.3880\n",
            "!!! NEW BEST MODEL (Epoch 7) !!! Saved ROUGE-L: 0.3880\n",
            "\n",
            "--- Starting Epoch 8 ---\n",
            "Epoch 8 - Training: 100% 1250/1250 [03:56<00:00,  5.28it/s, loss=3.27]\n",
            "--- Epoch 8 TRAIN finished --- Loss: 3.2711\n",
            "Epoch 8 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 8) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những điều quan trọng nhất đã diễn ra ở Trung Quốc , làm việc <unk> <unk> <unk> <unk> ở Tây Nam Chicago , năm ngoái , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 8 - Evaluating: 100% 125/125 [00:10<00:00, 12.10it/s]\n",
            "--- Epoch 8 EVAL finished --- Val Loss: 3.9724 | ROUGE-L: 0.3936\n",
            "!!! NEW BEST MODEL (Epoch 8) !!! Saved ROUGE-L: 0.3936\n",
            "\n",
            "--- Starting Epoch 9 ---\n",
            "Epoch 9 - Training: 100% 1250/1250 [04:01<00:00,  5.17it/s, loss=3.16]\n",
            "--- Epoch 9 TRAIN finished --- Loss: 3.1620\n",
            "Epoch 9 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 9) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những điều lớn nhất nhất đã được tìm ra ở Trung Quốc Anh , làm <unk> <unk> <unk> <unk> <unk> ở <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 9 - Evaluating: 100% 125/125 [00:09<00:00, 12.94it/s]\n",
            "--- Epoch 9 EVAL finished --- Val Loss: 3.9174 | ROUGE-L: 0.4034\n",
            "!!! NEW BEST MODEL (Epoch 9) !!! Saved ROUGE-L: 0.4034\n",
            "\n",
            "--- Starting Epoch 10 ---\n",
            "Epoch 10 - Training: 100% 1250/1250 [03:57<00:00,  5.26it/s, loss=3.07]\n",
            "--- Epoch 10 TRAIN finished --- Loss: 3.0661\n",
            "Epoch 10 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 10) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những sự kiện lớn nhất nhất trong khu vực Xê út , làm việc trong vòng 500 năm <unk> , <unk> , <unk> , <unk> , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 10 - Evaluating: 100% 125/125 [00:09<00:00, 12.67it/s]\n",
            "--- Epoch 10 EVAL finished --- Val Loss: 3.8883 | ROUGE-L: 0.4064\n",
            "!!! NEW BEST MODEL (Epoch 10) !!! Saved ROUGE-L: 0.4064\n",
            "\n",
            "--- Starting Epoch 11 ---\n",
            "Epoch 11 - Training: 100% 1250/1250 [03:59<00:00,  5.21it/s, loss=2.98]\n",
            "--- Epoch 11 TRAIN finished --- Loss: 2.9821\n",
            "Epoch 11 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 11) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những sự kiện nhất định nhất ở phía Nam , làm việc như một chiếc xe buýt lái xe ở <unk> <unk> <unk> ở <unk> , Missouri , <unk> .\n",
            "==================================================\n",
            "Epoch 11 - Evaluating: 100% 125/125 [00:10<00:00, 12.03it/s]\n",
            "--- Epoch 11 EVAL finished --- Val Loss: 3.8778 | ROUGE-L: 0.4115\n",
            "!!! NEW BEST MODEL (Epoch 11) !!! Saved ROUGE-L: 0.4115\n",
            "\n",
            "--- Starting Epoch 12 ---\n",
            "Epoch 12 - Training: 100% 1250/1250 [03:57<00:00,  5.27it/s, loss=2.9]\n",
            "--- Epoch 12 TRAIN finished --- Loss: 2.9028\n",
            "Epoch 12 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 12) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những điều tốt nhất là những người hâm mộ ở phía Nam Đại Tây Dương đã làm <unk> <unk> <unk> vào <unk> km vào năm 2001 , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 12 - Evaluating: 100% 125/125 [00:09<00:00, 12.55it/s]\n",
            "--- Epoch 12 EVAL finished --- Val Loss: 3.8497 | ROUGE-L: 0.4165\n",
            "!!! NEW BEST MODEL (Epoch 12) !!! Saved ROUGE-L: 0.4165\n",
            "\n",
            "--- Starting Epoch 13 ---\n",
            "Epoch 13 - Training: 100% 1250/1250 [03:59<00:00,  5.22it/s, loss=2.83]\n",
            "--- Epoch 13 TRAIN finished --- Loss: 2.8321\n",
            "Epoch 13 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 13) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những điều tốt nhất nhất trong phim màu da đen , làm việc tại Bangladesh , làm việc như một chiếc xe buýt lái xe ở phía bắc Bắc Carolina , Missouri , <unk> .\n",
            "==================================================\n",
            "Epoch 13 - Evaluating: 100% 125/125 [00:10<00:00, 11.95it/s]\n",
            "--- Epoch 13 EVAL finished --- Val Loss: 3.8460 | ROUGE-L: 0.4190\n",
            "!!! NEW BEST MODEL (Epoch 13) !!! Saved ROUGE-L: 0.4190\n",
            "\n",
            "--- Starting Epoch 14 ---\n",
            "Epoch 14 - Training: 100% 1250/1250 [03:59<00:00,  5.22it/s, loss=2.77]\n",
            "--- Epoch 14 TRAIN finished --- Loss: 2.7679\n",
            "Epoch 14 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 14) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những yếu tố lớn nhất đã được chụp vào bầu trời 500 Ấn Độ , làm <unk> <unk> <unk> <unk> ở Luân Đôn <unk> Street Street , <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 14 - Evaluating: 100% 125/125 [00:10<00:00, 12.06it/s]\n",
            "--- Epoch 14 EVAL finished --- Val Loss: 3.8411 | ROUGE-L: 0.4178\n",
            "No improvement. Patience: 1/10\n",
            "\n",
            "--- Starting Epoch 15 ---\n",
            "Epoch 15 - Training: 100% 1250/1250 [03:56<00:00,  5.29it/s, loss=2.71]\n",
            "--- Epoch 15 TRAIN finished --- Loss: 2.7076\n",
            "Epoch 15 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 15) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những điều tồi tệ nhất đã được kiểm tra tại Bangladesh Kentucky , làm việc trong phim <unk> <unk> <unk> tại <unk> , <unk> <unk> , <unk> .\n",
            "==================================================\n",
            "Epoch 15 - Evaluating: 100% 125/125 [00:10<00:00, 12.12it/s]\n",
            "--- Epoch 15 EVAL finished --- Val Loss: 3.8225 | ROUGE-L: 0.4224\n",
            "!!! NEW BEST MODEL (Epoch 15) !!! Saved ROUGE-L: 0.4224\n",
            "\n",
            "--- Starting Epoch 16 ---\n",
            "Epoch 16 - Training: 100% 1250/1250 [04:01<00:00,  5.18it/s, loss=2.65]\n",
            "--- Epoch 16 TRAIN finished --- Loss: 2.6532\n",
            "Epoch 16 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 16) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): Một trong những nhà sản xuất lớn nhất đã được kiểm tra tại Anh , làm việc trong phim 8 / 9 , làm <unk> <unk> <unk> <unk> <unk> trong hang <unk> <unk> , Missouri , <unk> .\n",
            "==================================================\n",
            "Epoch 16 - Evaluating: 100% 125/125 [00:10<00:00, 12.04it/s]\n",
            "--- Epoch 16 EVAL finished --- Val Loss: 3.8444 | ROUGE-L: 0.4228\n",
            "!!! NEW BEST MODEL (Epoch 16) !!! Saved ROUGE-L: 0.4228\n",
            "\n",
            "--- Starting Epoch 17 ---\n",
            "Epoch 17 - Training: 100% 1250/1250 [03:59<00:00,  5.22it/s, loss=2.6]\n",
            "--- Epoch 17 TRAIN finished --- Loss: 2.6018\n",
            "Epoch 17 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 17) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong số những đòn bẩy nhất định được được đưa ra ở Nam Á , <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> Street , tháng 6 , <unk> .\n",
            "==================================================\n",
            "Epoch 17 - Evaluating: 100% 125/125 [00:10<00:00, 12.07it/s]\n",
            "--- Epoch 17 EVAL finished --- Val Loss: 3.8279 | ROUGE-L: 0.4234\n",
            "!!! NEW BEST MODEL (Epoch 17) !!! Saved ROUGE-L: 0.4234\n",
            "\n",
            "--- Starting Epoch 18 ---\n",
            "Epoch 18 - Training: 100% 1250/1250 [03:58<00:00,  5.24it/s, loss=2.55]\n",
            "--- Epoch 18 TRAIN finished --- Loss: 2.5538\n",
            "Epoch 18 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 18) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): Một trong những người may mắn nhất , một trong những đòn bẩy nhất đã được kiểm tra ở Chicago , làm việc như một con ngựa trắng <unk> quét ở Tây Nam Chicago , Missouri , <unk> .\n",
            "==================================================\n",
            "Epoch 18 - Evaluating: 100% 125/125 [00:10<00:00, 12.04it/s]\n",
            "--- Epoch 18 EVAL finished --- Val Loss: 3.8467 | ROUGE-L: 0.4270\n",
            "!!! NEW BEST MODEL (Epoch 18) !!! Saved ROUGE-L: 0.4270\n",
            "\n",
            "--- Starting Epoch 19 ---\n",
            "Epoch 19 - Training: 100% 1250/1250 [04:00<00:00,  5.20it/s, loss=2.51]\n",
            "--- Epoch 19 TRAIN finished --- Loss: 2.5093\n",
            "Epoch 19 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 19) ========\n",
            "-> Source (EN):     <unk> <unk> , one of the most powerful storms ever recorded in the Atlantic Ocean , made <unk> as a <unk> 5 storm on Great <unk> Island in the northern <unk> on Sunday morning , September 1 , <unk> .\n",
            "-> Reference (VN):  Vào chủ nhật ngày <unk> , cơn bão <unk> , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió <unk> <unk> đổ bộ vào đảo Great <unk> , miền bắc <unk> .\n",
            "-> Prediction (VN): <unk> <unk> , một trong những điều tồi tệ nhất đã diễn ra trong phim da đen , làm việc như một người da đen đã bị cướp bóc bởi <unk> <unk> ở phía bắc , Missouri , <unk> .\n",
            "==================================================\n",
            "Epoch 19 - Evaluating: 100% 125/125 [00:10<00:00, 12.13it/s]\n",
            "--- Epoch 19 EVAL finished --- Val Loss: 3.8554 | ROUGE-L: 0.4283\n",
            "!!! NEW BEST MODEL (Epoch 19) !!! Saved ROUGE-L: 0.4283\n",
            "\n",
            "================= Final Test Evaluation =================\n",
            "Epoch 0 - Evaluating:   0% 0/125 [00:00<?, ?it/s]\n",
            "======== Example Translation (Epoch 0) ========\n",
            "-> Source (EN):     Brother Albert <unk> and his wife , <unk> Susan <unk> , from the West <unk> in <unk> , <unk>\n",
            "-> Reference (VN):  Anh Albert <unk> và chị Susan <unk> , thuộc hội thánh West ở <unk> , <unk>\n",
            "-> Prediction (VN): Charles Anh , ông ta đã làm <unk> <unk> , từ <unk> <unk> , từ <unk> <unk> , từ <unk> <unk> , <unk> đã kiểm soát kinh tế .\n",
            "==================================================\n",
            "Epoch 0 - Evaluating: 100% 125/125 [00:13<00:00,  9.36it/s]\n",
            "--- Epoch 0 EVAL finished --- Val Loss: 3.6437 | ROUGE-L: 0.4451\n",
            "Final Test Loss: 3.6437 | Test ROUGE-L: 0.4451\n",
            "Figure(1500x600)\n"
          ]
        }
      ]
    }
  ]
}